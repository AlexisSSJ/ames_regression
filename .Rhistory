RF_pipe = Pipeline(
[
('selector', prep),
('regressonr', rf)
]
)
RF_pipe.fit(ames_x_train, Sale_Price_train)
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first', sparse = False), cat_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
rf = RandomForestRegressor(
n_estimators=4,
min_samples_split=2,
min_samples_leaf=2,
random_state=12345)
RF_pipe = Pipeline(
[
('selector', prep),
('regressonr', rf)
]
)
RF_pipe.fit(ames_x_train, Sale_Price_train)
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first', sparse_output = False), cat_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
rf = RandomForestRegressor(
n_estimators=4,
min_samples_split=2,
min_samples_leaf=2,
random_state=12345)
RF_pipe = Pipeline(
[
('selector', prep),
('regressonr', rf)
]
)
RF_pipe.fit(ames_x_train, Sale_Price_train)
RF_pipe.predict(ames_x_test)
RF_pipe.predict(ames_x_test)
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first',handle_unknow = 'ignore', sparse_output = False), cat_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
rf = RandomForestRegressor(
n_estimators=4,
min_samples_split=2,
min_samples_leaf=2,
random_state=12345)
RF_pipe = Pipeline(
[
('selector', prep),
('regressonr', rf)
]
)
RF_pipe.fit(ames_x_train, Sale_Price_train)
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first',handle_unknomw = 'ignore', sparse_output = False), cat_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
rf = RandomForestRegressor(
n_estimators=4,
min_samples_split=2,
min_samples_leaf=2,
random_state=12345)
RF_pipe = Pipeline(
[
('selector', prep),
('regressonr', rf)
]
)
RF_pipe.fit(ames_x_train, Sale_Price_train)
RF_pipe.predict(ames_x_test)
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first',handle_unknonw = 'ignore', sparse_output = False), cat_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
rf = RandomForestRegressor(
n_estimators=4,
min_samples_split=2,
min_samples_leaf=2,
random_state=12345)
RF_pipe = Pipeline(
[
('selector', prep),
('regressonr', rf)
]
)
RF_pipe.fit(ames_x_train, Sale_Price_train)
RF_pipe.predict(ames_x_test)
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first',handle_unknown  = 'ignore', sparse_output = False), cat_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
rf = RandomForestRegressor(
n_estimators=4,
min_samples_split=2,
min_samples_leaf=2,
random_state=12345)
RF_pipe = Pipeline(
[
('selector', prep),
('regressonr', rf)
]
)
RF_pipe.fit(ames_x_train, Sale_Price_train)
RF_pipe.predict(ames_x_test)
ggplot(aes(x = RF_pipe.predict(ames_x_test), y = Sale_Price_test)) + geom_point()
cat_cols = ['MS_SubClass',
'MS_Zoning',
'Street',
'Alley',
'Lot_Shape',
'Land_Contour',
'Utilities',
'Lot_Config',
'Land_Slope',
'Neighborhood',
'Condition_1',
'Condition_2',
'Bldg_Type',
'House_Style',
'Overall_Cond',
'Roof_Style',
'Roof_Matl',
'Exterior_1st',
'Exterior_2nd',
'Mas_Vnr_Type',
'Exter_Cond',
'Foundation',
'Bsmt_Cond',
'Bsmt_Exposure',
'BsmtFin_Type_1',
'BsmtFin_Type_2',
'Heating',
'Heating_QC',
'Central_Air',
'Electrical',
'Functional',
'Garage_Type',
'Garage_Finish',
'Garage_Cond',
'Paved_Drive',
'Pool_QC',
'Fence',
'Misc_Feature',
# 'Mo_Sold',
# 'Sale_Type',
# 'Sale_Condition'
]
num_cols =[ 'Bedroom_AbvGr',
'BsmtFin_SF_1',
'BsmtFin_SF_2',
'Bsmt_Full_Bath',
'Bsmt_Half_Bath',
'Bsmt_Unf_SF',
'Enclosed_Porch',
'Fireplaces',
'First_Flr_SF',
'Full_Bath',
'Garage_Area',
'Garage_Cars',
'Gr_Liv_Area',
'Half_Bath',
'Kitchen_AbvGr',
'Latitude',
'Longitude',
'Lot_Area',
'Lot_Frontage',
'Mas_Vnr_Area',
'Misc_Val',
'Open_Porch_SF',
'Pool_Area',
'Screen_Porch',
'Second_Flr_SF',
'Three_season_porch',
'TotRms_AbvGrd',
'Total_Bsmt_SF',
'Wood_Deck_SF',
'Year_Built',
'Year_Remod_Add',
# 'Year_Sold',
'area_per_car',
'last_remod',
'wood_prop']
# Definir las métricas de desempeño que deseas calcular como funciones de puntuación
def adjusted_r2_score(y_true, y_pred, n, p):
r2 = r2_score(y_true, y_pred)
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
return adjusted_r2
# Definir el objeto K-Fold Cross Validator
k = 5
kf = KFold(n_splits=k, shuffle=True, random_state=42)
def adjusted_r2_score(y_true, y_pred, n, p):
r2 = r2_score(y_true, y_pred)
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
return adjusted_r2
# Definir el objeto K-Fold Cross Validator
k = 7
kf = KFold(n_splits=k, shuffle=True, random_state=42)
param_grid = {
'max_depth': range(2, 20),
'min_samples_split': range(2, 15),
'min_samples_leaf': range(2, 15),
'max_features': range(1, 50)
}
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(adjusted_r2_score, greater_is_better=True,
n=np.ceil(len(ames_train_selected)), p=len(ames_train_selected.columns)),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first',handle_unknown  = 'ignore', sparse_output = False), cat_cols),
('selector' , 'passthrough', num_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
pipeline = Pipeline([
('preprocessor', prep),
('regressor', GridSearchCV(
RandomForestRegressor(),
param_grid = param_grid,
cv=kf,
scoring=scoring,
refit='neg_mean_squared_error',
verbose=2,
n_jobs=7,
error_score='raise')
)
])
pipeline.fit(ames_train_selected, ames_y_train)
pickle.dump(pipeline, open('models/grid_search_random_forest.pkl', 'wb'))
pipeline = Pipeline([
('preprocessor', prep),
('regressor', GridSearchCV(
RandomForestRegressor(),
param_grid = param_grid,
cv=kf,
scoring=scoring,
refit='neg_mean_squared_error',
verbose=2,
n_jobs=7,
error_score='raise')
)
])
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(adjusted_r2_score, greater_is_better=True,
n=np.ceil(len(ames_train_selected)), p=len(ames_train_selected.columns)),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first',handle_unknown  = 'ignore', sparse_output = False), cat_cols),
('selector' , 'passthrough', num_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
pipeline = Pipeline([
('preprocessor', prep),
('regressor', GridSearchCV(
RandomForestRegressor(),
param_grid = param_grid,
cv=kf,
scoring=scoring,
refit='neg_mean_squared_error',
verbose=2,
n_jobs=7,
error_score='raise')
)
])
pipeline.fit(ames_train_selected, ames_y_train)
pickle.dump(pipeline, open('models/grid_search_random_forest.pkl', 'wb'))
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(adjusted_r2_score, greater_is_better=True,
n=np.ceil(len(ames_train_selected)), p=len(ames_train_selected.columns)),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_x_train)), p=len(num_cols+cat_cols)),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first',handle_unknown  = 'ignore', sparse_output = False), cat_cols),
('selector' , 'passthrough', num_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
pipeline = Pipeline([
('preprocessor', prep),
('regressor', GridSearchCV(
RandomForestRegressor(),
param_grid = param_grid,
cv=kf,
scoring=scoring,
refit='neg_mean_squared_error',
verbose=2,
n_jobs=7,
error_score='raise')
)
])
pipeline.fit(ames_train_selected, ames_y_train)
pickle.dump(pipeline, open('models/grid_search_random_forest.pkl', 'wb'))
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_x_train)), p=len(num_cols+cat_cols)),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first',handle_unknown  = 'ignore', sparse_output = False), cat_cols),
('selector' , 'passthrough', num_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
pipeline = Pipeline([
('preprocessor', prep),
('regressor', GridSearchCV(
RandomForestRegressor(),
param_grid = param_grid,
cv=kf,
scoring=scoring,
refit='neg_mean_squared_error',
verbose=2,
n_jobs=7,
error_score='raise')
)
])
pipeline.fit(ames_train_selected, ames_y_train)
pickle.dump(pipeline, open('models/grid_search_random_forest.pkl', 'wb'))
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_x_train)), p=len(num_cols+cat_cols)),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first',handle_unknown  = 'ignore', sparse_output = False), cat_cols),
('selector' , 'passthrough', num_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
pipeline = Pipeline([
('preprocessor', prep),
('regressor', GridSearchCV(
RandomForestRegressor(),
param_grid = param_grid,
cv=kf,
scoring=scoring,
refit='neg_mean_squared_error',
verbose=2,
n_jobs=7,
error_score='raise')
)
])
pipeline.fit(ames_train_selected, ames_y_train)
pickle.dump(pipeline, open('models/grid_search_random_forest.pkl', 'wb'))
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_x_train)), p=len(num_cols+cat_cols)),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first',handle_unknown  = 'ignore', sparse_output = False), cat_cols),
('selector' , 'passthrough', num_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
pipeline = Pipeline([
('preprocessor', prep),
('regressor', GridSearchCV(
RandomForestRegressor(),
param_grid = param_grid,
cv=kf,
scoring=scoring,
refit='neg_mean_squared_error',
verbose=2,
n_jobs=7,
error_score='raise')
)
])
pipeline.fit(ames_train_selected, ames_y_train)
pickle.dump(pipeline, open('models/grid_search_random_forest.pkl', 'wb'))
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_x_train)), p=len(num_cols+cat_cols)),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first',handle_unknown  = 'ignore', sparse_output = False), cat_cols),
('selector' , 'passthrough', num_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
pipeline = Pipeline([
('preprocessor', prep),
('regressor', GridSearchCV(
RandomForestRegressor(),
param_grid = param_grid,
cv=kf,
scoring=scoring,
refit='neg_mean_squared_error',
verbose=2,
n_jobs=7,
error_score='raise')
)
])
pipeline.fit(ames_train_selected, ames_y_train)
pickle.dump(pipeline, open('models/grid_search_random_forest.pkl', 'wb'))
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_x_train)), p=len(num_cols+cat_cols)),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first',handle_unknown  = 'ignore', sparse_output = False), cat_cols),
('selector' , 'passthrough', num_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
pipeline = Pipeline([
('preprocessor', prep),
('regressor', GridSearchCV(
RandomForestRegressor(),
param_grid = param_grid,
cv=kf,
scoring=scoring,
refit='neg_mean_squared_error',
verbose=2,
n_jobs=7,
error_score='raise')
)
])
pipeline.fit(ames_train_selected, ames_y_train)
pickle.dump(pipeline, open('models/grid_search_random_forest.pkl', 'wb'))
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_x_train)), p=len(num_cols+cat_cols)),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first',handle_unknown  = 'ignore', sparse_output = False), cat_cols),
('selector' , 'passthrough', num_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
pipeline = Pipeline([
('preprocessor', prep),
('regressor', GridSearchCV(
RandomForestRegressor(),
param_grid = param_grid,
cv=kf,
scoring=scoring,
refit='neg_mean_squared_error',
verbose=2,
n_jobs=7,
error_score='raise')
)
])
pipeline.fit(ames_train_selected, ames_y_train)
pickle.dump(pipeline, open('models/grid_search_random_forest.pkl', 'wb'))
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_x_train)), p=len(num_cols+cat_cols)),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
prep = ColumnTransformer([
('dummy', OneHotEncoder(drop = 'first',handle_unknown  = 'ignore', sparse_output = False), cat_cols),
('selector' , 'passthrough', num_cols)],
remainder = 'drop',
verbose_feature_names_out = False).set_output(transform = 'pandas')
pipeline = Pipeline([
('preprocessor', prep),
('regressor', GridSearchCV(
RandomForestRegressor(),
param_grid = param_grid,
cv=kf,
scoring=scoring,
refit='neg_mean_squared_error',
verbose=2,
n_jobs=7,
error_score='raise')
)
])
pipeline.fit(ames_train_selected, ames_y_train)
pickle.dump(pipeline, open('models/grid_search_random_forest.pkl', 'wb'))
prep.get_feature_names_out()
prep.fit(ames_x_train)get_feature_names_out()
prep.fit(ames_x_train).get_feature_names_out()
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_x_train)), p=len(prep.fit(ames_x_train).get_feature_names_out())),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
pipeline = Pipeline([
('preprocessor', prep),
('regressor', GridSearchCV(
RandomForestRegressor(),
param_grid = param_grid,
cv=kf,
scoring=scoring,
refit='neg_mean_squared_error',
verbose=2,
n_jobs=7,
error_score='raise')
)
])
pipeline.fit(ames_x_train, Sale_Price_train)
