---
title: "EDA"                    
author: "Uriel Alexis Luna López"
format: html                    
editor: visual                  
---

# Analisís de las variables

## Descripción de las variables

### Variables continuas y númericas

| Variable                    | Descrición                                                                           |
|----------------------|:-------------------------------------------------|
| Lot_Frontage (Continua)     | Loguitud que conectan la calle con la propiedad (en pies)                            |
| Lot Area (Continua)         | Tamaño del lote en pies cuadrados                                                    |
| Mas Vnr Area (Continua)     | Área de revestimiento de mampostería en pies cuadrados                               |
| BsmtFin SF 1 (Continua)     | Area del sótano terminada Tipo 1 en pies cuadrados                                   |
| BsmtFin SF 2 (Continua)     | Area del sótano terminada Tipo 2 en pies cuadrados                                   |
| Bsmt Unf SF (Continua)      | Área sin terminar del sótano en pies cuadrados                                       |
| Total Bsmt SF (Continua)    | Área total del sótano                                                                |
| 1st Flr SF (Continua)       | Pies cuadrqados del primer piso                                                      |
| 2nd Flr SF (Continua)       | Pies cuadrados del segundo piso                                                      |
| Low Qual Area SF (Continua) | Pies cuadrados terminados de baja calidad                                            |
| Gr Liv Area (Continua)      | Superficie habitable sobre el nivel del suelo en pies cuadrados                      |
| Wood Deck SF (Continua)     | Área de cubierta de madera en pies cuadrados                                         |
| Open Porch SF (Continua)    | Área abierta del porche en pies cuadrados                                            |
| Enclose Porch (Continua)    | Área cerrada del proche en pies cuadrados                                            |
| 3-Ssn Porch (Continua)      | Área de porche de tres estaciones en pies cuadrados                                  |
| Screen Porch (Continua)     | Área del porche cubierto en pies cuadrados                                           |
| Pool Area (Continua)        | Área de la piscina en pies cuadrados                                                 |
| Misc Val (Continua)         | Valor de la característica miscelanea                                                |
| **Sale_Price (Continua)**   | Precio de venta.                                                                     |
| Year Built (Discreta)       | Año original de construcción                                                         |
| Year Remod/Add (Discreta)   | Fecha de remodelación (la misma que la fecha de construcción si no se ha remodelado) |
| Bsmt Full Bath (Discreta)   | Baños completos en el sótano                                                         |
| Bsmt Half Bath (Discreta)   | Medios baños en el sótano                                                            |
| Full Bath (Discreta)        | Baños completos sobre el nivel del suelo                                             |
| Half Bath (Discreta)        | Medios baños sobre el nivel del suelo                                                |
| Kitchen (Discreta)          | Concinas sobre el nivel del suelo                                                    |
| TotRmsAbvGrd                | Total de habitaciones sobre el nivel del suelo (no incluye baños)                    |
| Fireplaces (Discreta)       | Número de chimeneas                                                                  |
| Garage Yr Blt (Discreta)    | Año en que se construyó el garage                                                    |
| Garage Cars (Discreta)      | Tamaño del garage en cuanto a capacidad para autos.                                  |
| Mo Sold (Discreta)          | Mes de la venta (MM)                                                                 |
| Yr Sold (Discreta)          | Año de la venta (YYYY)                                                               |

### Varaibles categóricas

| Variable                 | Descrición                                                                             |
|---------------------|---------------------------------------------------|
| MS_Subclass (Nominal)    | Identifica el tipo de vivienda de la venta                                             |
| MS_Zoning (Nominal)      | Identifica la clasificación de la zona asociada a la venta                             |
| Street (Nominal)         | Tipo de camino de acceso a la propiedad                                                |
| Alley (Nominal)          | Tipo de callejón de acceso a la propiedad                                              |
| Lot Contour (Nominal)    | Planitud de la propiedad                                                               |
| Lot Config (Nominal)     | Configurarición del lote                                                               |
| Neightboorhood (Nominal) | Ubicación física dentro de los límites de la ciudad de Ames                            |
| Condition 1 (Nominal)    | Proximidad a varias condiciones                                                        |
| Condition 2 (Nominal)    | Proximidad a varias condiciones (Si más de una es presente)                            |
| Bldg Type (Nominal)      | Tipo de vivienda                                                                       |
| House Style (Nominal)    | Estilo de la vivienda                                                                  |
| Roof Style (Nominal)     | Tipo de techo                                                                          |
| Roof Matl (Nominal)      | Material del techo                                                                     |
| Exterior 1 (Nominal)     | Revestimiento exterior de la casa                                                      |
| Exterior 2 (Nominal)     | Revestimiento exterior de la casa (Si hay más de un material)                          |
| Mas Vnr Type (Nominal)   | Tipo de revestimiento de mampostería                                                   |
| Foundation (Nominal)     | Tipo de cimentación                                                                    |
| Heating (Nominal)        | Tipo de calefacción                                                                    |
| Central Air (Nominal)    | Aire acondicionado centralizado                                                        |
| Garage Type (Nominal)    | Ubicación del garage                                                                   |
| Misc Feature (Nominal)   | Característica miscelánea no cubierta en otras categorías                              |
| Sale Type (Nominal)      | Tipo de venta                                                                          |
| Sale Condition (Nominal) | Condicón de la venta.                                                                  |
| Functional (Ordinal)     | Funcionalidad del hogar (suponga que es típico a menos que se justifiquen deducciones) |
| FireplaceQu (Ordinal)    | Calidad de las chimeneas                                                               |
| Lot Shape (Ordinal)      | Forma general de la propiedad                                                          |
| Pool QC (Ordinal)        | Calidad de la piscina                                                                  |
| Utilities (Ordinal)      | Tipo de utilidades disponibles                                                         |
| Land Slope (Ordinal)     | Pendiente de la propiedad                                                              |
| Overall Qual (Ordinal)   | Califica el material general y el acabado de la casa.                                  |
| Overall Cond (Ordinal)   | Califica la condición general de la casa                                               |
| Exter Qual (Ordinal)     | Evalua la calidad materiales de los exteriores                                         |
| Exter Cond (Ordinal)     | Evalua la condición presente del materia en el exterior                                |
| Bsmt Qua (Ordinal)       | Evalua la altura del sótano                                                            |
| Bsmt Cond (Ordinal)      | Evalua la condición general del sótano                                                 |
| Bsmt Exposure (Ordinal)  | Se refiere a paredes a nivel de jardín o de salida                                     |
| BsmtFin Type 1 (Ordinal) | Calificación del area terminada del sótano                                             |
| BsmtFin Type 2 (Ordinal) | Clasificación del área terminada del sótano (si hay varios tipos)                      |
| Heating QC (Ordinal)     | Calidad y condición de la calefacción                                                  |
| Electrical (Ordinal)     | Sistema electrico                                                                      |
| Kitchen Qaul (Ordinal)   | Calidad de la cocina                                                                   |
| Garage Qual (Ordinal)    | Calidad del Garage                                                                     |
| Garage cond (Ordinal)    | Condicón del garage                                                                    |
| Paved Drive (Ordinal)    | Camino pavimentado.                                                                    |
| Garage Finish (Ordnal)   | El interior del garage esta terminado                                                  |
| Fence (Ordinal)          | Calidad de la cerca.                                                                   |

```{r}
#| label: startPy
#| include: false
library(reticulate)

use_virtualenv("CD_AMAT_2023")
```

```{python}
#| label: load-py-packages
#| include: false

import pandas as pd
from siuba import *
import numpy as np
from plotnine import *
import plydata as pr
from plydata.tidy import pivot_wider, pivot_longer
import matplotlib.pyplot as plt
```


```{r}
py_run_file('Custom_functions_ ames.py')
```

## Analísis de Sale Price

A continuación se muestra el split inicial

```{python}
#| label: intial_split
#| echo: true

from sklearn.model_selection import train_test_split
from plydata.one_table_verbs import pull
from mizani.formatters import comma_format, dollar_format




ames = pd.read_csv("../data/ames.csv") >> mutate (Price_4_GLA= _.Sale_Price/_.Gr_Liv_Area)

y = ames >>  pull("Price_4_GLA")
X = select(ames, -_.Sale_Price, -_.Price_4_GLA)

X=(X >> mutate(wood_prop= _.Wood_Deck_SF/_.Gr_Liv_Area,
              area_per_car=case_when({
                              _.Garage_Cars == 0:0,
                              True:_.Garage_Area/ _.Garage_Cars}),
              last_remod = case_when({
                              _.Year_Built>_.Year_Remod_Add: 2023-_.Year_Remod_Add,
                              True: 2023-_.Year_Built}),
              Mo_Sold= _.Mo_Sold.astype('object')))

numeric_column = ames >> pull("Price_4_GLA")
quartiles = np.percentile(numeric_column, [25, 50, 75])

# Crea una nueva variable categórica basada en los cuartiles
stratify_variable = pd.cut(
 numeric_column, 
 bins=[float('-inf'), quartiles[0], quartiles[1], quartiles[2], float('inf')],
 labels=["Q1", "Q2", "Q3", "Q4"]
 )

ames_x_train, ames_x, ames_y_train, ames_y = train_test_split(
                       X, y, 
                       test_size = 0.30, 
                       random_state = 12345, 
                       stratify = stratify_variable
                       )

stratify_variable = pd.cut(
 ames_y , 
 bins=[float('-inf'), quartiles[0], quartiles[1], quartiles[2], float('inf')],
 labels=["Q1", "Q2", "Q3", "Q4"]
 )

ames_x_test, ames_x_val, ames_y_test, ames_y_val = train_test_split(
       ames_x, ames_y, 
       test_size = 1/3, 
       random_state = 12345, 
       stratify = stratify_variable
       )
       
Sale_Price_train = ames_y_train * ames_x_train.Gr_Liv_Area
Sale_Price_test = ames_y_test * ames_x_test.Gr_Liv_Area
Sale_Price_validation = ames_y_val * ames_x_val.Gr_Liv_Area
print('Totales:\t', ames.shape[0],'\nTraining:\t', ames_y_train.shape[0],'\nTesting:\t', ames_y_test.shape[0],'\nValidation:\t', ames_y_val.shape[0])
 
```

Cómo se observa, la distribución de los precios de venta se encuentra sesgada a la derecha, tienendo el 75% de las casas vendidas a un precio menor a \$212,075 y aclumulando un la mitad de estos entre \$129K y 212K. También en el siguiente qqplot podemos ver que la distribución de los precios de venta no parece seguir una distribución normal.

```{python}
#| output: false
#| echo: false

summary = "count: 2051\nmean: $179,122.64\nstd: $79,337.45\nQ1: $128,225\nQ2:$159,500\nQ3: $211,000\nmin: $12,789\nmax:$755,000"
```

```{python}
#| results: hide
#| code-fold: true
#| label: sale_price_dist
#| fig-keep: 'all'

# plt.figure()
(
  ggplot(aes(y=Sale_Price_train, x=0))+
                geom_violin( fill='#702C27',
                                style='right',
                                alpha=0.6)+
                geom_jitter(aes(x=np.zeros(2051)-0.21),
                            size=1,
                            alpha=0.4,
                            color='#006DC8',
                            width=0.2,
                            random_state=7)+
                geom_boxplot(aes(x=np.zeros(2051)-0.21),width=0.4,
                                    color='#182E40',
                                    alpha=0.1,
                                    outlier_alpha=1,
                                    outlier_color='#235C6C',
                                    outlier_size=1.5)+
                coord_flip()+
                theme_classic()+
                ggtitle('Distribución de los precios de venta')+
                scale_y_continuous(labels=dollar_format(big_mark=','))+
                ylab('Precio de venta')+
                annotate('text',x=0.3,y=500000, label=summary,ymax=400000)+
                geom_text(ha='left',label=summary, x=0.3, y=500000, size=10, fontweight=1)+
                theme(axis_ticks_minor_y=element_blank(),
                        axis_ticks_major_y=element_blank(),
                        axis_text_y=element_blank()
                        )+
                theme(figure_size=(8, 6))
)

```

```{python}
#| label: qqplot_Sale_Price
#| echo: false
#| results: hide

params={'loc':Sale_Price_train.mean(), 'scale':Sale_Price_train.std()}

(
                ggplot(aes(sample=Sale_Price_train))+
                geom_qq(alpha = 0.3,distribution='norm', dparams=params)+
                stat_qq_line(distribution='norm', dparams=params,color = "red")+
                scale_y_continuous(labels=dollar_format(big_mark=',', digits = 0)) + 
                scale_x_continuous(labels=dollar_format(big_mark=',', digits = 0)) + 
                xlab("Cuantilies de la dist. normal teórica") + ylab("Cuantiles de Sale Price") +
                ggtitle("QQ-Plot N~(mu=180,996.79, sd=81,186.69)")+
                theme_seaborn()
            
)
```

A diferencia de la distribución de los precios se venta de la casa, el precio por pie cuadrado habitable de la casa parece tener un comportamiento más normal:

```{python}
#| results: hide
#| code-fold: true
#| label: sale_price_GLA_DIST
#| fig-keep: 'all'
summary = "count: 2344\nmean: $180,996.79\nstd: $81,186.69\nQ1: $129,500\nQ2:$160,000\nQ3: $213,492.50\nmin: $12,789\nmax:$755,000"
# plt.figure()
(
  ggplot(aes(y=ames_y_train, x=0))+
                geom_violin( fill='#702C27',
                                style='right',
                                alpha=0.6)+
                geom_jitter(aes(x=np.zeros(2051)-0.21),
                            size=1,
                            alpha=0.4,
                            color='#006DC8',
                            width=0.2,
                            random_state=7)+
                geom_boxplot(aes(x=np.zeros(2051)-0.21),width=0.4,
                                    color='#182E40',
                                    alpha=0.1,
                                    outlier_alpha=1,
                                    outlier_color='#235C6C',
                                    outlier_size=1.5)+
                coord_flip()+
                theme_classic()+
                ggtitle('Distribución de los precios de venta')+
                scale_y_continuous(labels=dollar_format(big_mark=','))+
                ylab('Precio de venta')+
                # annotate('text',x=0.3,y=500000, label=summary,ymax=400000)+
                geom_text(ha='left',label=summary, x=0.3, y=500000, size=10, fontweight=1)+
                theme(axis_ticks_minor_y=element_blank(),
                        axis_ticks_major_y=element_blank(),
                        axis_text_y=element_blank()
                        )+
                theme(figure_size=(8, 6))
)

```

```{python}
#| results: hide
#| code-fold: true
#| label: qq plot new y
#| fig-keep: 'all'
params={'loc':ames_y_train.mean(), 'scale':ames_y_train.std()}

(
                ggplot(aes(sample=ames_y_train))+
                geom_qq(alpha = 0.3,distribution='norm', dparams=params)+
                stat_qq_line(distribution='norm', dparams=params,color = "red")+
                scale_y_continuous(labels=dollar_format(big_mark=',', digits = 0)) + 
                scale_x_continuous(labels=dollar_format(big_mark=',', digits = 0)) + 
                xlab("Cuantilies de la dist. normal teórica") + ylab("Cuantiles de Sale Price") +
                ggtitle("QQ-Plot N~(mean=180,996.79, sd=81,186.69)")+
                theme_seaborn()
            
)
```

## Variables Continuas VS Transformación de Sale_Price

```{python}
#| echo: false
continuas = [   'Lot_Frontage',
                'Lot_Area', 
                'Mas_Vnr_Area', 
                'Sale_Price/Gr_Liv_Area',
                'BsmtFin_SF_1', 
                'BsmtFin_SF_2', 
                'Bsmt_Unf_SF', 
                'Total_Bsmt_SF', 
                'First_Flr_SF', 
                'Second_Flr_SF', 
                # 'Low_Qual_Area_SF', 
                'Gr_Liv_Area', 
                'Wood_Deck_SF', 
                'Open_Porch_SF', 
                'Enclosed_Porch', 
                'Three_season_porch', 
                'Screen_Porch', 
                'Pool_Area', 
                'Misc_Val']

```

```{python}
#| label: corr_heatmap_HCC
#| results: hide
#| code-fold: true
#| fig-keep: 'all'

ames_train = ames_x_train.copy()
ames_train['Sale_Price/Gr_Liv_Area'] = ames_y_train
correlation_data = (
    ames_train[continuas].corr().melt(  ignore_index=False,
                                        var_name='var1').reset_index() >>
                    select( _.var1,
                            _.var2 == _.index,
                            _.cor == _.value  ) 
)


labels=(
    correlation_data >> case_when({_.cor.between(-0.3, 0.3):'',
                                    _.cor>0.99991: '',
                                    True :_.cor.round(2).astype(str)})
)

(
    correlation_data >>
        arrange(-_.cor,_.var1) >>
        # filter (_.var1!=_.var2) >>
        mutate(labb=labels)>>
        ggplot(aes(x='var1', y='var2', fill= 'cor'))+
        geom_tile()+
        geom_label(aes(label='labb'), size=6)+
        scale_fill_gradientn(colors=['#0092ff','#e6d6c7','#FF1400'],
                            limits=(-1,1),
                            name='Correlación')+
        ggtitle('Mapa de calor de correlaciones para variables continuas')+
        xlab('')+ylab('')+#guides(fill=guide_legend(title="New Legend Title"))+
        theme(axis_text_x=element_text(rotation=90,hjust = -1),
                figure_size=(8,6))
)


```

Como podemos observar para ser que las varaibles de `'BsmtFin_SF_1', 'First_Flr_SF', 'Second_Flr_SF', 'Total_Bsmt_SF'` son las que tienen correlaciones más grandes con el precio de venta por el precio por pie cuadrado habitable. En el gráfico sólo se muestran etiquetas de correlaciones que no estan en el intervalo (-0.3.0.3).

```{python}
#| label: corr_sp/gla
#| results: hide
#| code-fold: true
#| fig-keep: 'all'
(
    ames_train[continuas].corr().reset_index(drop=False) 
    >> select(_['Sale_Price/Gr_Liv_Area'],_.index)
    >> filter(_['Sale_Price/Gr_Liv_Area'] != 1)
    >> mutate (labels = np.round(_['Sale_Price/Gr_Liv_Area'], 3))
    >> arrange(_['Sale_Price/Gr_Liv_Area'])
    >> ggplot(aes(y='Sale_Price/Gr_Liv_Area', x='index', fill='Sale_Price/Gr_Liv_Area'))
    + geom_col()
    + geom_text(aes(label = 'labels') , size=8, va='center', angle=90)
    + ylim([-0.5,0.5])
    + labs(title = 'Sale_Price/Gr_Liv_Area vs otras variables', y='', x='')
    + scale_fill_gradient2(low='#4EB043',
                           mid='#B05F43',
                           high= '#3A3AD5',
                           midpoint=0,
                           limits=(-1,1),
                           name='Correlación')
    + theme_bw()
    + theme(axis_text_x=element_text(angle=90))
)
```

## Precios dependiendo la ubicación

```{python}
#| label: price por ubi
#| results: hide
#| code-fold: true
#| fig-keep: 'all'
colors=['#EC00FF' ,'#0028F1','#00F104','#FF2D00']
(ames_train >> ggplot(aes(x='Latitude', y='Longitude', color='Sale_Price/Gr_Liv_Area')) +
                geom_point(alpha=0.5, size=1.5, stroke=1, shape='2')+
                labs(title='Precios dependiendo de la ubicación geografica', 
                    colour='Precio de venta', 
                    y='Longuitud',
                    x= 'Latidud')+
                scale_color_gradientn(colors=colors,labels=dollar_format(big_mark=','))+ theme_538()
 )


```

```{python}
#| label: error_bars por vecindario
#| results: hide
#| code-fold: true
#| fig-keep: 'all'
(
  ames_train 
        >> group_by(_.Neighborhood) 
        >> summarize(media = np.mean(_['Sale_Price/Gr_Liv_Area']), desv=np.std(_['Sale_Price/Gr_Liv_Area']))
        >> mutate( max_err = _.media  + _.desv, min_err =  _.media  - _.desv)
        >> arrange(_.media)
        >> ggplot(aes(x='reorder(Neighborhood,media)', y='media') ) 
        + geom_errorbar(aes(ymin='min_err', ymax='max_err'),width=0.3, position=position_dodge(0.9))+geom_point()
        + theme_bw()
        + theme(axis_text_x=element_text(angle=90))
        + scale_y_continuous(labels=dollar_format(big_mark=',', digits = 0)) 
        +coord_flip()
        +labs(y='Precio  promedio por pie cuadrado de área habitable', x='')
)
```

## Variables relacionadas con el sótano

Se puede ver que se tiene una relación líneal creciente entre `Total_Bsmt_SF` y `Sale_Price\ Gr_Liv_Area`, en la siguiente gráfica tambien podemos ver como varios puntos se acumulan en `Total_Bsmt_SF = 0` debido a que estas casas no cuentan con sótano

```{python}
#| label: price vs sf sotano
#| results: hide
#| code-fold: true
#| fig-keep: 'all'
(
    ames_x_train 
        >> ggplot( aes(y=ames_y_train, x= 'Total_Bsmt_SF'))
        + geom_point(size=1,alpha=0.4)
        + geom_smooth(color='red')
        + scale_y_continuous(labels = dollar_format(digits=0, big_mark=',') )
        +xlim([0,2500])
        + labs(title = 'Total_Bsmt_SF vs  Sale Price/ Gr_Liv_Area', x='Total_Bsmt_SF', y='Sale Price/ Gr_Liv_Area')
        + theme_538()
)
```

En cuanto a las variables categóricas relacionadas al sótano encontramos lo siguiente:

```{python}
#| label: analisis sotano
#| results: hide
#| code-fold: true
#| fig-keep: 'all'
cat_columns = pd.Series(['Bldg_Type', 
            'Bsmt_Exposure', 
            'Central_Air', 
            'Condition_1', 
            'Condition_2', 
            'Electrical', 
            'Fence', 
            'Foundation', 
            'Garage_Finish', 
            'Garage_Type', 
            'Misc_Feature', 
            'Paved_Drive', 
            'Heating_QC',
            'Overall_Cond',
            'MS_SubClass', 
            'MS_Zoning',
            'Bsmt_Cond',
            'BsmtFin_Type_1', 
            'BsmtFin_Type_2',
            'House_Style',
            'Lot_Shape',
            'Neighborhood'])
for category in cat_columns[cat_columns.str.contains('Bsmt')]:
  titulo='Categoría analazada: {}'.format(category) 
  (ames_train  >> ggplot(aes(y= 'Sale_Price/Gr_Liv_Area', x= 'Total_Bsmt_SF', color= '{}'.format(category)))
              + geom_point(alpha= 0.5, show_legend=False) + geom_smooth(color='black')
              +xlim([0,3000])
              +labs(x='Área total del sótano', y='Precio por pie cuadrado de área habitable bruta', title=titulo)
              + facet_wrap('~{}'.format(category))+theme_538 ()) 
```

Puntos a considerar:

-   En `Bsmt_Exposure` se puede notar que la relación positiva entre las variables se mantiene, y no parece cambiar si se altera la exposicón del sótano. Además se muestran elementos con `Total_Bmst_SF`\>0 que caen en la categoría de No basement, lo cual puede deberse a un error.

-   En `Bsmt_Cond` se puede ver que la relación de las variables si cambia dependiendo de la condición del sótano, además en la condición de Excellent y Poor se tienen muy pocas observaciones.

-   En `Bsmt_Type_1` se tienen relaciones demaciado planas en su mayoria, a excepción de GLQ y Unf.

-   En `Bsmt_Type_2` se tiene el mismo caso que en el anterior. Además se muestran elementos con `Total_Bmst_SF`\>0 que caen en la categoría de No basement, lo cual puede deberse a un error.

# Regresión Líneal

```{python}
#| label: libraries lm
#| echo: false

from mlxtend.feature_selection import ColumnSelector
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder,FunctionTransformer
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate
import statsmodels.api as sm 
```

```{python}
#| label: funciones creadas lm
#| results: hide
#| code-fold: true
pd.options.display.float_format = '{:.2f}'.format
# 
# def importance_from_model (test_frame, y_obs, selected_columns, pipeline, actual_mse, n_permutations=50 ,trans_pred= False ):
#   performance_losses = []
#   
#   for i in range(test_frame[selected_columns].shape[1]):
#     loss = []
#     for j in range(n_permutations):
#         test_frame_permuted = test_frame[selected_columns].copy()
#         test_frame_permuted.iloc[:, i] = np.random.permutation(test_frame_permuted.iloc[:, i])
#         if trans_pred == False:
#             y_pred_permuted = pipeline.predict(test_frame_permuted)
#         else:
#             y_pred_permuted = pipeline.predict(test_frame_permuted)* test_frame_permuted.Gr_Liv_Area
#         mse_permuted = mean_squared_error(y_obs, y_pred_permuted)
#         loss.append(mse_permuted)
#     performance_losses.append(loss)
#   
#   performance_losses = performance_losses/np.sum(performance_losses, axis=0)
#   mean_losses = np.mean(performance_losses, axis=1)
#   std_losses = np.std(performance_losses, axis=1)
#   
#   importance_df = pd.DataFrame({
#   'Variable': selected_columns, 
#   'Mean_Loss': mean_losses, 
#   'Std_Loss': std_losses
#   })
#   return importance_df
#   
# def adjusted_r2_score(y_true, y_pred, n, p):
#   r2 = r2_score(y_true, y_pred)
#   adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
#   return adjusted_r2 
# 
# 
# def div_columns(X, c1, c2):
#     X["c1_c2"] = X[c1]/ X[c2]
#     return X
# 
# 
# 
# def adj_r(y_o, y_p , p, n):
#   r2_adj = 1 - (n - 1) / (n - p - 1) * (1 - r2_score(y_o, y_p))
#   
#   
# def get_metrics (y_pred, y_obs,predictors):
#   me = np.mean(y_obs - y_pred)
#   mae = mean_absolute_error(y_obs, y_pred)
#   mape = mean_absolute_percentage_error(y_obs, y_pred)
#   mse = mean_squared_error(y_obs, y_pred)
#   rmse = np.sqrt(mse)
#   r2 = r2_score(y_obs, y_pred)
# 
#   n = len(y_obs)  # Número de observaciones
#   p = predictors  # Número de predictores 
#   r2_adj = 1 - (n - 1) / (n - p - 1) * (1 - r2)
#   
# 
#   metrics_data = {
#       "Metric": ["ME", "MAE", "MAPE", "MSE", "RMSE", "R2", "R2Adj"],
#       "Value": [me, mae, mape, mse, rmse, r2, r2_adj]
#   }
#   return pd.DataFrame(metrics_data).set_index('Metric')
# 
# 
# 
# def rmv_elements (list_of_elements, complete_list):
#   for element in list_of_elements:
#     complete_list.remove(element)
#   return complete_list
# 
# 
# 
# def validation_results(x_val,y_val,  pipeline , n_preds):
#   y_pred = pipeline.predict(x_val)
#   
#   ames_test = (
#     ames_x_val >>
#     mutate(Sale_Price_Pred = y_pred*_.Gr_Liv_Area, Sale_Price =y_val*_.Gr_Liv_Area))
#   
#   ##### Métricas de desempeño
#   
#   y_obs = ames_test["Sale_Price"]
#   y_pred = ames_test["Sale_Price_Pred"]
#   return get_metrics(y_pred, y_obs, n_preds)
# 

```

A continuación se muestra el modelo de regresión líneal que tuvo un mejor performance de acuerdo a las variables que se estudiaron:

```{python}
#| label: resultaqdos lm test
#| code-fold: true
num_cols =sorted( ['Wood_Deck_SF', 
            'First_Flr_SF',
            'Fireplaces',
            'Open_Porch_SF',
            'Enclosed_Porch',
            'Lot_Area',
            'wood_prop',
            'Total_Bsmt_SF',
            'Garage_Area',
            'Gr_Liv_Area',
            'area_per_car',
            'last_remod',
            'Bsmt_Full_Bath', 
            'Three_season_porch',
            'BsmtFin_SF_1', 
            'BsmtFin_SF_2', 
            'Bsmt_Unf_SF',
            'Bsmt_Half_Bath', 
            'Full_Bath',
            'Kitchen_AbvGr',
            'Half_Bath',
            'Mas_Vnr_Area',
            'Misc_Val',
            'Lot_Frontage',
            'Bedroom_AbvGr'])

cat_cols = ['Bldg_Type', 
            'Bsmt_Exposure', 
            'Central_Air','Mo_Sold', 
            'Condition_1', 
            'Condition_2', 
            'Electrical', 
            'Mas_Vnr_Type',
            'Fence', 
            'Exter_Cond',
            'Foundation', 
            'Garage_Finish', 
            'Garage_Type', 
            # 'Misc_Feature', 
            'Paved_Drive', 
            'Heating_QC',
            'Overall_Cond',
            'MS_SubClass', 
            'Bsmt_Cond',
            'BsmtFin_Type_1', 
            'BsmtFin_Type_2', 
            'House_Style',
            'Lot_Shape', #'Roof_Style'
            'Neighborhood']
            
columnas_seleccionadas = num_cols + sorted(cat_cols + ['Year_Built','Garage_Cars','Misc_Feature'])
 
#######################################################################
preprocessor_1 = ColumnTransformer(
    transformers = [
        ('scaler', StandardScaler(), num_cols),
        ('imputer',  SimpleImputer(missing_values=np.nan, strategy='mean'), ['Year_Built','Garage_Cars']),
        ('OHE', OneHotEncoder(drop='first',handle_unknown='ignore' , sparse_output=False, min_frequency=20), cat_cols),
        ('OHE_1', OneHotEncoder(drop='first',handle_unknown='ignore' , sparse_output=False), ['Misc_Feature'])
    ],
    verbose_feature_names_out = False,
    remainder = 'drop'  
    ).set_output(transform = 'pandas')
```

```{python}
from sklearn.preprocessing import PolynomialFeatures

interaction_transformer = PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)
interaction_transformer_wb = PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)
```

```{python}
drop_cols=['Neighborhood_Crawford',
            'Neighborhood_infrequent_sklearn',
            'BsmtFin_Type_1_BLQ',
            'Exter_Cond_Good',
            # 'Condition_2_Feedr',
            'Condition_1_RRAe',
            # 'Condition_1_Feedr',
            'Garage_Type_No_Garage',
            # 'Condition_1_PosA',
            'Bsmt_Full_Bath',
            'Bsmt_Half_Bath',
            # 'Condition_2_RRAe',
            'Foundation_CBlock',
            'MS_SubClass_Two_Story_1946_and_Newer',
            'Garage_Finish_No_Garage',
            'Bldg_Type_TwoFmCon',
            'Mo_Sold_5',
            'Paved_Drive_Partial_Pavement',
            'Bsmt_Cond_Typical',
            'Heating_QC_Fair',
            'BsmtFin_Type_1_Unf',
            'Electrical_SBrkr',
            'Fence_Good_Wood',
            # 'Electrical_Unknown',
            'Garage_Type_Detchd',
            'Foundation_Slab',
            'House_Style_infrequent_sklearn',
            'Three_season_porch',
            'BsmtFin_Type_2_GLQ',
            'Mo_Sold_7',
            'Misc_Val',
            'BsmtFin_Type_1_No_Basement',
            'Foundation_PConc',
            'Mo_Sold_12',
            'Mo_Sold_10',
            'BsmtFin_Type_2_No_Basement',
            'last_remod',
            'Bsmt_Cond_No_Basement',
            'Condition_2_infrequent_sklearn',
            'Exter_Cond_Typical',
            'Bldg_Type_OneFam',
            'Mo_Sold_9',
            'Electrical_infrequent_sklearn',
            'Bsmt_Cond_infrequent_sklearn',
            'Bsmt_Cond_Good',
            'Foundation_infrequent_sklearn',
            'Bldg_Type_TwnhsE',
            'Fence_No_Fence']

  

preprocessor_2=ColumnTransformer(
  transformers=[
    ("selector", "drop", drop_cols),
    ('interaction_1', interaction_transformer_wb, ['Lot_Area', 'Gr_Liv_Area']),
    ('interactions2', interaction_transformer, ['Year_Built', 'Overall_Cond_Average']),
    ('interactions3', interaction_transformer, ['Full_Bath', 'Bedroom_AbvGr']),
    ('interactions3.1', interaction_transformer, ['House_Style_SLvl', 'Overall_Cond_Fair']),
    ('interactions3.4', interaction_transformer, ['wood_prop', 'Mas_Vnr_Area']),
    ('interactions3.2', interaction_transformer, ['First_Flr_SF', 'last_remod']),
    ('interactions3.5', interaction_transformer, ['BsmtFin_SF_1', 'BsmtFin_Type_2_Unf']),
    ('interactions4', interaction_transformer, ['Misc_Val', 'Misc_Feature_TenC'])
  ],
  verbose_feature_names_out = False,
  remainder='passthrough'
).set_output(transform = 'pandas')

prep_pipe = Pipeline([
  ('prep1',preprocessor_1),
  ('selec_inter', preprocessor_2)
])
  
##### Extracción de coeficientes
transformed_df = prep_pipe.fit_transform(ames_x_train)
# 
X_train_with_intercept = sm.add_constant(transformed_df)
model = sm.OLS(ames_y_train, X_train_with_intercept).fit()

model.summary()

```

```{python}
pipeline = Pipeline([
  ('preprocessor', preprocessor_1),
  ('select_interac', preprocessor_2),
  ('regressor', LinearRegression())])

# Entrenar el pipeline
results = pipeline.fit(ames_x_train, ames_y_train)

## PREDICCIONES
y_pred = pipeline.predict(ames_x_test)

ames_test = (
  ames_x_test >>
  mutate(Sale_Price_Pred = y_pred*_.Gr_Liv_Area, Sale_Price =ames_y_test*_.Gr_Liv_Area)
)

##### Métricas de desempeño

y_obs = ames_test["Sale_Price"]
y_pred = ames_test["Sale_Price_Pred"]

predictores = transformed_df.columns.to_list()
mtrcs_dt =get_metrics(y_pred, y_obs,predictors= len(transformed_df.columns ))


metrics_df = pd.DataFrame(mtrcs_dt)
print(metrics_df)

```

El modelo muestra una $R^2_{adj}=0.85$ con los datos de test, y tienen un error aproximado del $\pm10\%$ con respecto del precio real.

```{python}
y_pred = pipeline.predict(ames_x_val)*ames_x_val.Gr_Liv_Area

get_metrics(y_pred,Sale_Price_validation,len(transformed_df.columns ) )
```

En cuanto al análisis de los residuos, no parece seguir completamente los supuestos, pues visualmete se puede detectar heterocedasticidad, lo cual puede afectar el preformance del modelo.

```{python}
#### Gráficos de desempeño de modelo

(
    ggplot(aes(x = y_obs, y = y_pred)) +
    geom_point(alpha=0.5) +
    scale_y_continuous(labels = dollar_format(digits=0, big_mark=','), limits = [0, 600000] ) +
    scale_x_continuous(labels = dollar_format(digits=0, big_mark=','), limits = [0, 600000]) +
    geom_abline(color = "red") +
    coord_equal() +
    labs(
      title = "Comparación entre predicción y observación",
      x = "Predicción",
      y = "Observación")
)


(
ames_test >>
  select(_.Sale_Price, _.Sale_Price_Pred) >>
  mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
  ggplot(aes(x = "error")) +
  geom_histogram(color = "white", fill = "black") +
  geom_vline(xintercept = 0, color = "red") +
  scale_x_continuous(labels=dollar_format(big_mark=',', digits=0)) + 
  ylab("Conteos de clase") + xlab("Errores") +
  ggtitle("Distribución de error")
)


(
ames_test >>
  select(_.Sale_Price, _.Sale_Price_Pred) >>
  mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
  ggplot(aes(sample = "error")) +
  geom_qq(alpha = 0.3) + stat_qq_line(color = "red") +
  scale_y_continuous(labels=dollar_format(big_mark=',', digits = 0)) + 
  xlab("Distribución normal") + ylab("Distribución de errores") +
  ggtitle("QQ-Plot")
)


(
ames_test >>
  select(_.Sale_Price, _.Sale_Price_Pred) >>
  mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
  ggplot(aes(x = "Sale_Price")) +
  geom_linerange(aes(ymin = 0, ymax = "error"), colour = "purple") +
  geom_point(aes(y = "error"), size = 0.05, alpha = 0.5) +
  geom_abline(intercept = 0, slope = 0) +
  scale_x_continuous(labels=dollar_format(big_mark=',', digits=0)) + 
  scale_y_continuous(labels=dollar_format(big_mark=',', digits=0)) +
  xlab("Precio real") + ylab("Error de estimación") +
  ggtitle("Relación entre error y precio de venta")
)
```

# Regresión con Knn
## Cross validation con todas las variables
Se hace un cross validation con todas las features que se tienen, em busca de los hiperparámetros adecuados para el modelo, al menos de manera provisional
```{python}
#| label: libs_knn
#| echo: false
from mlxtend.feature_selection import ColumnSelector
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate
from sklearn.model_selection import GridSearchCV
from sklearn.utils import shuffle

from plydata.one_table_verbs import pull
from plydata.tidy import pivot_longer
from mizani.formatters import comma_format, dollar_format
from plotnine import *
from siuba import *

import pandas as pd
import numpy as np
from sklearn.ensemble import VotingRegressor
```


```{python}
#| label: column_cat_num_divisor
#| echo: false
cat_cols = ['MS_SubClass', 
                'MS_Zoning',
                'Street', 
                'Alley', 
                'Lot_Shape',
                'Land_Contour', 
                'Utilities', 
                'Lot_Config',
                'Land_Slope', 
                'Neighborhood',
                'Condition_1',
                'Condition_2', 
                'Bldg_Type', 
                'House_Style',
                'Overall_Cond', 
                'Roof_Style', 
                'Roof_Matl',
                'Exterior_1st',
                'Exterior_2nd', 
                'Mas_Vnr_Type', 
                'Exter_Cond',
                'Foundation', 
                'Bsmt_Cond',
                'Bsmt_Exposure', 
                'BsmtFin_Type_1', 
                'BsmtFin_Type_2', 
                'Heating',
                'Heating_QC', 
                'Central_Air', 
                'Electrical', 
                'Functional', 
                'Garage_Type',
                'Garage_Finish', 
                'Garage_Cond', 
                'Paved_Drive', 
                'Pool_QC', 
                'Fence',
                'Misc_Feature',
                'Mo_Sold', 
                # 'Sale_Type', 
                # 'Sale_Condition'
                ]

num_cols =[ 'Bedroom_AbvGr',
            'BsmtFin_SF_1',
            'BsmtFin_SF_2',
            'Bsmt_Full_Bath',
            'Bsmt_Half_Bath',
            'Bsmt_Unf_SF',
            'Enclosed_Porch',
            'Fireplaces',
            'First_Flr_SF',
            'Full_Bath',
            'Garage_Area',
            'Garage_Cars',
            'Gr_Liv_Area',
            'Half_Bath',
            'Kitchen_AbvGr',
            'Latitude',
            'Longitude',
            'Lot_Area',
            'Lot_Frontage',
            'Mas_Vnr_Area',
            'Misc_Val',
            'Open_Porch_SF',
            'Pool_Area',
            'Screen_Porch',
            'Second_Flr_SF',
            'Three_season_porch',
            'TotRms_AbvGrd',
            'Total_Bsmt_SF',
            'Wood_Deck_SF',
            'Year_Built',
            'Year_Remod_Add',
            # 'Year_Sold',
            'area_per_car',
            'last_remod',
            'wood_prop']  
preprocessor= ColumnTransformer([
  ('std', StandardScaler(), num_cols),
  ('onehotencoding', OneHotEncoder(drop='first',handle_unknown = 'ignore', sparse_output=False, min_frequency=15), cat_cols)
  ],
  remainder = 'drop', 
  verbose_feature_names_out= False).set_output(transform = 'pandas')
```

### Preprocesado de las variables 
 Para el modelo de KNN se tomará como variable predictiva el precio de venta, sin ninguna transformación. 
  -   Variables númericas:
      _   Estadarización
  -     Variables categóricas:
      - One hot econding, tomando un mínimo de 15 observaciones en las categorías
```{python}
#| eval: false

preprocessor= ColumnTransformer([
  ('std', StandardScaler(), num_cols),
  ('onehotencoding', OneHotEncoder(drop='first',handle_unknown = 'ignore', sparse_output=False, min_frequency=15), cat_cols)
  ],
  remainder = 'drop', 
  verbose_feature_names_out= False).set_output(transform = 'pandas')

predictors = preprocessor.fit_transform(ames_x_train).columns

scoring = {
    'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
    'r2': make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_x_train)), p=len(predictors)),
    'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
    'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}



kf = KFold(n_splits=10, shuffle=True, random_state=7)


param_grid = {
 'n_neighbors': range(0, 70, 5),
 'weights': ['uniform', 'distance'],
 'metric': ['euclidean', 'manhattan', 'cosine'],

}
    
    
gs_knn_fm_cv = Pipeline([
  ('prep', preprocessor),
  ('cv', GridSearchCV(
      KNeighborsRegressor(), 
      param_grid, 
      cv=kf, 
      scoring=scoring, 
      refit='neg_mean_squared_error',
      verbose=3, 
      n_jobs=7))
])

gs_knn_fm_cv.fit(ames_x_train, Sale_Price_train )
```

```{python}
#| echo: false
# pd.to_pickle(gs_knn_fm_cv, "first_knn_cv_.pkl")
gs_knn_fm_cv = pd.read_pickle('first_knn_cv_.pkl')
```

### Resultados CV
En la siguiente gráfica se puede ver el las estimaciones  de las metricas de $R^2_{adj},\space RMSE\text{ y } MAPE$. La distancia de $manhattan$  parece ser la que tiene mejores resultados en las tres métricas, y el hiper-parámetro $K$ muestra mejores cuando es menor a 25. 

```{python}
#| label: plot_knn_cv1
#| fig-keep: all
#| code-fold: true
cv_results = pd.DataFrame(gs_knn_fm_cv.named_steps['cv'].cv_results_)

cv_results = (cv_results 
              >> select(-_.contains('split'),-_.params, -_.contains('time'))
              >> mutate(RMSE = abs(_.mean_test_neg_mean_squared_error)**0.5)
             >> pivot_longer(
                  cols=['mean_test_r2', 'mean_test_mape','RMSE'],
                  names_to='parameter', 
                  values_to='value') >> mutate(value = abs(_.value))
                  )


(
  cv_results 
    >> ggplot( aes(x = "param_n_neighbors", y = "value", shape = "param_weights", color= 'param_metric')) 
    + geom_point(alpha = 0.9,position=position_dodge(width=0.1))
    + facet_wrap("~parameter",ncol =1, scales = "free_y")
    + labs( y = '',x= 'Parámetro: vecinos cercanos K' ,shape = 'Ponderación',color = 'Métrica' )
)



```
### Párametros óptimos para el full model

```{python}
#| eval: false
param_grid = {
 'n_neighbors': range(3, 26),
 'weights': ['uniform'],
 'metric': ['euclidean', 'manhattan', 'cosine']
}
    
    
gs_knn_fm_cv2 = Pipeline([
  ('prep', preprocessor),
  ('cv', GridSearchCV(
      KNeighborsRegressor(), 
      param_grid, 
      cv=kf, 
      scoring=scoring, 
      refit='neg_mean_squared_error',
      verbose=3, 
      n_jobs=-1))
])

gs_knn_fm_cv2.fit(ames_x_train, Sale_Price_train )
```

```{python}
#| echo: false
# pd.to_pickle(gs_knn_fm_cv2, 'second_knn_cv_.pkl')
gs_knn_fm_cv2 = pd.read_pickle('second_knn_cv_.pkl')
```

```{python}
cv_results = pd.DataFrame(gs_knn_fm_cv2.named_steps['cv'].cv_results_)

cv_results = (cv_results 
              >> select(-_.contains('split'),-_.params, -_.contains('time'))
              >> mutate(RMSE = abs(_.mean_test_neg_mean_squared_error)**0.5)
             >> pivot_longer(
                  cols=['mean_test_r2', 'mean_test_mape','RMSE'],
                  names_to='parameter', 
                  values_to='value') >> mutate(value = abs(_.value))
                  )


(
  cv_results 
    >> ggplot( aes(x = "param_n_neighbors", y = "value", shape = "param_weights", color= 'param_metric')) 
    + geom_point(alpha = 1)
    + facet_wrap("~parameter",ncol =1, scales = "free_y")
    + labs( y = '',x= 'Parámetro: vecinos cercanos K' ,shape = 'Ponderación',color = 'Métrica' )
)
```
Analizando los diferentes resultados se opta por un parametro de $K=10$ pues muestra una buena estimación tanto para $R^2_{adj}$ y $MAPE$, además, esta elección nos proporcionara un modelo más robusto que otras opciones que parecen arrojar mejores métricas.

```{python}
(
  cv_results
      >> mutate( 
        error_min = case_when({
          _.parameter == 'mean_test_r2': _.value - _.std_test_r2,
          _.parameter == 'mean_test_mape': _.value - _.std_test_mape, True:1
        }),
        error_max = case_when({
          _.parameter == 'mean_test_r2': _.value + _.std_test_r2,
          _.parameter == 'mean_test_mape': _.value + _.std_test_mape, True:1
                                })
        )
      >> filter(_.param_n_neighbors.astype(int) <=15, 
                _.parameter != 'RMSE',
                _.param_metric == 'manhattan') 
      >> ggplot( aes(x = "param_n_neighbors", y = "value", color= 'param_metric')) 
      + geom_point(alpha = 1,position=position_dodge(width=0.7))
      + geom_errorbar(aes(ymin='error_min', ymax='error_max'),
      position=position_dodge(width=0.7),show_legend=False)
      + facet_wrap("~parameter",ncol =1, scales = "free_y")
      + labs( y = '',x= 'Parámetro: vecinos cercanos K' ,shape = 'Ponderación',color = 'Métrica' )
)
```


## Feature selection
Se hara un analísis del preformance del modelo en test, y se analizará la importancia de las variables y se buscara hara una selección de variables de acuerdo a eso mismo.El  modelo muestra los siguientes resultados:


```{python}
knn_reg = KNeighborsRegressor(
    n_neighbors = 10,
    weights = 'uniform',
    metric ='manhattan')
    
knn_= Pipeline([
  ('prep',preprocessor),
  ('regressor',knn_reg )
  ])
    
knn_.fit(ames_x_train , Sale_Price_train)

y_pred  = knn_.predict(ames_x_test)
predictors = preprocessor.get_feature_names_out()

metrics = get_metrics(y_pred, Sale_Price_test, len(predictors))

print(metrics)
```
A continucación se calcula la importancia de variables para el full model
```{python}
#| eval: false
importance_df = importance_from_model(
  test_frame = preprocessor.transform(ames_x_test) ,
  y_obs = Sale_Price_test ,
  selected_columns = list(predictors), 
  pipeline = knn_reg,
  actual_mse = metrics.loc['MSE'].values[0] ,
  n_permutations= 50,
  trans_pred= False
)
```
```{python}
#| echo: false
# importance_df.to_csv( 'importance_knn_1_.csv')
importance_df = pd.read_csv('importance_knn_1_.csv') 
pd.options.display.float_format = '{:.5f}'.format
```
Al tener muchas variables, el efecto en los shuffles es muy pequeño para todas las variables, pero aún as;i es comparable entre ellas, a continuación se muestran los resultados.
```{python}
importance_df.iloc[:,1:].head(10)
```
En las siguientes gráficas se pueden observar las 15 variables que más afectan al $MSE$, es decir, las que se podrían considerar más importatntes, y también se muestran las 15 variables con menos impacto al modelo.
```{python}
#| fig-keep: all
#| code-fold: true
(
  importance_df >> top_n(15, _.Mean_Loss) >>
  mutate(
    ymin = _.Mean_Loss - _.Std_Loss,
    ymax = _.Mean_Loss + _.Std_Loss) >>
  ggplot(aes(x = 'reorder(Variable, Mean_Loss)', y = "Mean_Loss")) +
  geom_errorbar(aes(ymin='ymin', ymax='ymax'),
    width=0.2, position=position_dodge(0.9)) +
  geom_point(alpha = 0.65) +
  labs(title='Importancia de las Variables (TOP 15)', x='Variable', y='Importancia') +
  coord_flip()
)
(
  importance_df >> top_n(-15, _.Mean_Loss)>>
  mutate(
    ymin = _.Mean_Loss - _.Std_Loss,
    ymax = _.Mean_Loss + _.Std_Loss) >>
  ggplot(aes(x = 'reorder(Variable, Mean_Loss)', y = "Mean_Loss")) +
  geom_errorbar(aes(ymin='ymin', ymax='ymax'),
    width=0.2, position=position_dodge(0.9)) +
  geom_point(alpha = 0.65) +
  labs(title='Importancia de las Variables (Peores 15)', x='', y='Importancia') +
  coord_flip()
)

```
Para poder hacer selección de features, se probaran 200 modelos, donde se irán eliminando variables de acuerdo a su importancia, de la menos importante hacia las que más importancia tienen, posteriormente se elegira el mejor de esos 150 modelos
```{python}
#| eval: false
vars_importance_order = list( importance_df >> arrange(_.Mean_Loss) >> pull('Variable'))
transformed_df = preprocessor.fit_transform(ames_x_train)
transformed_test_df = preprocessor.transform(ames_x_test)
v=[]
r2=[]
mse_=[]
p=[]
for i in range(200):
  vars_to_train = vars_importance_order[i:]
  p.append(len(vars_to_train))
  knn_reg.fit(transformed_df[vars_to_train], Sale_Price_train)
  pred= knn_reg.predict(transformed_test_df[vars_to_train])
  mse_.append(get_metrics(pred,Sale_Price_test, len(vars_to_train)).loc['MSE'].values[0])
  r2.append(get_metrics(pred,Sale_Price_test, len(vars_to_train)).loc['R2Adj'].values[0])
  v.append(vars_to_train)
idf = pd.DataFrame({'MSE':mse_, 'R2_adj':r2,'vars':v, 'predictors':p})
idf.to_csv('iters_knn_feature_selection.csv')
best_features_from_fm = (idf >> mutate(MSE= (_.MSE)**(0.5) ) >> filter(_.MSE < 33500) >> top_n(1, _.R2_adj) >> pull('vars'))[0]
```


```{python}
#| echo: false
pd.options.display.float_format = '{:.2f}'.format
idf = pd.read_csv('iters_knn_feature_selection.csv')
best_features_from_fm = (idf >> mutate(MSE= (_.MSE)**(0.5) ) >> filter(_.MSE < 33500) >> top_n(1, _.R2_adj) >> pull('vars'))[0]
(idf.iloc[:,1:] >> select(-_.vars)).head()
```
Podemos ver que el mejor modelo podría encontrarse entre 50 y 75 predictores, de acuerdo a los modelos que se probaron, pues entre ese inrevalo el MSE toma valores menores y la $R^2_{adj}$ se mantiene en valores por encima de 0.8.
```{python}
(
  idf 
    >> pivot_longer(
        cols=['MSE','R2_adj' ],names_to='variables', values_to='value'
                  )
    >> ggplot(aes(x ='predictors', y='value'))
    +  geom_point(alpha = 0.5)
    +  facet_wrap('~variables', ncol=1, scales ='free_y')
)
```
Haciendo un zoom podemos ver que el mejor candidato es el modelo con 63 predictores
```{python}
(
  idf 
    >> filter  (_.predictors.between(10,63))
    >> pivot_longer(
        cols=['MSE','R2_adj' ],names_to='variables', values_to='value'
                  )
    >> ggplot(aes(x ='predictors', y='value'))
    +  geom_point(alpha = 0.5)
    +  facet_wrap('~variables', ncol=1, scales ='free_y')
)
```

```{python}
#| label: best_vars_knn and selector def
#| echo: false

idf >> filter(_.predictors == 20) >> pull('vars')
bm_features =  ['Second_Flr_SF', 'Lot_Area', 'Neighborhood_Timberland', 'Garage_Type_BuiltIn', 'Latitude', 'Half_Bath', 'Screen_Porch', 'Year_Remod_Add', 'Garage_Cars', 'TotRms_AbvGrd', 'Neighborhood_Stone_Brook', 'Full_Bath', 'Garage_Area', 'Open_Porch_SF', 'Bsmt_Full_Bath', 'Mas_Vnr_Area', 'Total_Bsmt_SF', 'Fireplaces', 'Gr_Liv_Area', 'First_Flr_SF']
selectorr = ColumnTransformer(
  [('selector', 'passthrough',bm_features)],
  remainder = 'drop',
  verbose_feature_names_out= False).set_output(transform = 'pandas')
```

```{python}
#| eval: false

preprocessor= ColumnTransformer([
  ('std', StandardScaler(), num_cols),
  ('onehotencoding', OneHotEncoder(drop='first',handle_unknown = 'ignore', sparse_output=False, min_frequency=15), cat_cols)
  ],
  remainder = 'drop', 
  verbose_feature_names_out= False).set_output(transform = 'pandas')

selectorr = ColumnTransformer(
  [('selector', 'passthrough',bm_features)],
  remainder = 'drop',
  verbose_feature_names_out= False).set_output(transform = 'pandas')



kf = KFold(n_splits=10, shuffle=True, random_state=77)


param_grid = {
 'n_neighbors': range(5, 33, 1),
 'weights': ['uniform', 'distance'],
 'metric': ['euclidean', 'manhattan', 'cosine' ],
}
    
    
final_cv_knn = Pipeline([
  ('prep', preprocessor),
  ('selector', selectorr),
  ('cv', GridSearchCV(
      KNeighborsRegressor(), 
      param_grid, 
      cv=kf, 
      scoring=scoring, 
      refit='neg_mean_squared_error',
      verbose=3, 
      n_jobs=7))
])

final_cv_knn.fit(ames_x_train, Sale_Price_train )

```


```{python}
#| echo: false

# pd.to_pickle(final_cv_knn, 'knn_cv_after_feature_s.pkl')
final_cv_knn = pd.read_pickle('knn_cv_after_feature_s.pkl')
```


```{python}
cv_results = pd.DataFrame(final_cv_knn.named_steps['cv'].cv_results_)

cv_results = (cv_results 
              >> select(-_.contains('split'),-_.params, -_.contains('time'))
              >> mutate(RMSE = abs(_.mean_test_neg_mean_squared_error)**0.5)
              >> pivot_longer(
                  cols=['mean_test_r2', 'mean_test_mape','RMSE'],
                  names_to='parameter', 
                  values_to='value') >> mutate(value = abs(_.value))
                  )


(
  cv_results 
    >>filter(_.param_metric != 'minkowski')
    >> ggplot( aes(x = "param_n_neighbors", y = "value", shape = "param_weights", color= 'param_metric')) 
    + geom_point(alpha = 0.9,position=position_dodge(width=0.1))
    + facet_wrap("~parameter",ncol =1, scales = "free_y")
    + labs( y = '',x= 'Parámetro: vecinos cercanos K' ,shape = 'Ponderación',color = 'Métrica' )
)
```

```{python}
knn_reg = KNeighborsRegressor(
    n_neighbors = 14,
    weights = 'distance',
    metric ='manhattan')

f_num_knn_reg_fm = Pipeline([
  ('prep',preprocessor),
  ('selector', selectorr ),
  ('regressor', knn_reg )
  ])

f_num_knn_reg_fm .fit(ames_x_train, Sale_Price_train)

y_pred = f_num_knn_reg_fm .predict(ames_x_test)
get_metrics(y_pred, Sale_Price_test, len (bm_features))

validation_results(x_val=ames_x_val,pipeline=f_num_knn_reg_fm,y_val=Sale_Price_validation,n_preds = len (bm_features))
```


```{python}
y_obs = Sale_Price_test
ames_test = (
  ames_x_test >>
  mutate(Sale_Price_Pred = y_pred, Sale_Price = Sale_Price_test)
)
(
    ggplot(aes(x = y_obs, y = y_pred)) +
    geom_point(alpha=0.5) +
    scale_y_continuous(labels = dollar_format(digits=0, big_mark=','), limits = [0, 600000] ) +
    scale_x_continuous(labels = dollar_format(digits=0, big_mark=','), limits = [0, 600000]) +
    geom_abline(color = "red") +
    coord_equal() +
    labs(
      title = "Comparación entre predicción y observación",
      x = "Predicción",
      y = "Observación")
)


(
ames_test >>
  select(_.Sale_Price, _.Sale_Price_Pred) >>
  mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
  ggplot(aes(x = "error")) +
  geom_histogram(color = "white", fill = "black") +
  geom_vline(xintercept = 0, color = "red") +
  scale_x_continuous(labels=dollar_format(big_mark=',', digits=0)) + 
  ylab("Conteos de clase") + xlab("Errores") +
  ggtitle("Distribución de error")
)


(
ames_test >>
  select(_.Sale_Price, _.Sale_Price_Pred) >>
  mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
  ggplot(aes(sample = "error")) +
  geom_qq(alpha = 0.3) + stat_qq_line(color = "red") +
  scale_y_continuous(labels=dollar_format(big_mark=',', digits = 0)) + 
  xlab("Distribución normal") + ylab("Distribución de errores") +
  ggtitle("QQ-Plot")
)


(
ames_test >>
  select(_.Sale_Price, _.Sale_Price_Pred) >>
  mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
  ggplot(aes(x = "Sale_Price")) +
  geom_linerange(aes(ymin = 0, ymax = "error"), colour = "purple") +
  geom_point(aes(y = "error"), size = 0.05, alpha = 0.5) +
  geom_abline(intercept = 0, slope = 0) +
  scale_x_continuous(labels=dollar_format(big_mark=',', digits=0)) + 
  scale_y_continuous(labels=dollar_format(big_mark=',', digits=0)) +
  xlab("Precio real") + ylab("Error de estimación") +
  ggtitle("Relación entre error y precio de venta")
)
```

# Random Forest

```{python}
from sklearn.ensemble import RandomForestRegressor
rf_gs = pd.read_pickle('grid_search_random_forest.pkl')


```



```{python}
cv_results = pd.DataFrame(rf_gs.named_steps['regressor'].cv_results_)
cv_results = (cv_results 
              >> select(-_.contains('split|tiime'))
              >> mutate(RMSE = abs(_.mean_test_neg_mean_squared_error)**0.5)
             >> pivot_longer(
                  cols=['mean_test_r2', 'mean_test_mape','RMSE'],
                  names_to='parameter', 
                  values_to='value') >> mutate(value = abs(_.value))
                  )


(
  cv_results 
    >> ggplot( aes(x = "param_max_features", y = "value", color ='param_min_samples_leaf' )) 
    + geom_jitter(alpha = 0.5,size=0.7)
    + facet_wrap("~parameter",ncol =1, scales = "free_y")
)
```

```{python}
(
  cv_results >> filter(_.param_min_samples_leaf <7, _.param_max_features >30)
    >> ggplot( aes(x = "param_max_features", y = "value", color ='param_min_samples_leaf' )) 
    + geom_boxplot(alpha = 0.5,size=0.7)
    + facet_wrap("~parameter",ncol =1, scales = "free_y")
)
```
```{python}
(
  cv_results >> filter(_.param_min_samples_leaf <6, _.param_max_features >30)
    >> ggplot( aes(x = "param_max_depth", y = "value")) 
    + geom_boxplot(alpha = 0.5,size=0.7)
    + facet_wrap("~parameter",ncol =1, scales = "free_y")
)
```


```{python}
cv_results = pd.DataFrame(rf_gs.named_steps['regressor'].cv_results_)

(
  cv_results >> filter ( _.mean_test_r2>0.8)>>
  select(_.param_max_depth, _.param_max_features, _.param_min_samples_leaf, 
         _.param_min_samples_split, _.mean_test_r2) >>
  pivot_longer(
    cols = ["param_max_depth", "param_max_features", "param_min_samples_leaf", "param_min_samples_split"],
    names_to="parameter",
    values_to="value") >>
  ggplot(aes(x = "value", y = "mean_test_r2")) +
  geom_point(size = 0.7, alpha = 0.5) +
  facet_wrap("~parameter", scales = "free_x") +
  xlab("Parameter value") +
  ylab("R^2 promedio") +
  ggtitle("Parametrización de Random Forest vs R^2")
)
```

```{python}




prep = ColumnTransformer([
    ('dummy', OneHotEncoder(drop = 'first',handle_unknown  = 'ignore', sparse_output = False), cat_cols),
    ('selector' , 'passthrough', num_cols)],
    remainder = 'drop',
    verbose_feature_names_out = False).set_output(transform = 'pandas')

rf_model =  rf_gs.named_steps['regressor'].best_estimator_
    
rf_pipe = Pipeline([
  ('preprocesado', prep),
  ('rf', rf_model)
])


rf_pipe.fit(ames_x_train, Sale_Price_train)
```

```{python}
y_pred = rf_pipe.predict(ames_x_test)
p = len(rf_pipe.named_steps['preprocesado'].get_feature_names_out())
actual_mse = get_metrics(y_pred, Sale_Price_test, p).iloc[3,0]
```

## importance

```{python}
#| eval: false
transformed_test_df = rf_pipe.named_steps['preprocesado'].transform(ames_x_test)

importance_df = importance_from_model(
  test_frame = transformed_test_df ,
  y_obs = Sale_Price_test,
  selected_columns = list(rf_pipe.named_steps['preprocesado'].transform(ames_x_test).columns) ,
  pipeline = rf_model,
  actual_mse = actual_mse,
  n_permutations = 50,
  trans_pred = False)
importance_df.to_csv('importance_rf_1.csv')
```

```{python}
#| eval: false
transformed_test_df = rf_pipe.named_steps['preprocesado'].transform(ames_x_test)
vars_importance_order = list( importance_df >> arrange(_.Mean_Loss) >> pull('Variable'))
transformed_df = rf_pipe.named_steps['preprocesado'].transform(ames_x_train)
v=[]
r2=[]
mse_=[]
p=[]
for i in range(260):
  vars_to_train = vars_importance_order[i:]
  
  mf = max(55-i, round(len(vars_to_train)**0.5))
  
  rf = RandomForestRegressor(max_depth=13, max_features=mf, min_samples_leaf=3,min_samples_split=6, random_state=8)
  
  p.append(len(vars_to_train))
  
  
  rf.fit(transformed_df[vars_to_train], Sale_Price_train)
  pred = rf.predict(transformed_test_df[vars_to_train])
  
  mse_.append(get_metrics(pred,Sale_Price_test, len(vars_to_train)).loc['MSE'].values[0])
  r2.append(get_metrics(pred,Sale_Price_test, len(vars_to_train)).loc['R2Adj'].values[0])
  v.append(vars_to_train)
  
  
idf = pd.DataFrame({'MSE':mse_, 'R2_adj':r2,'vars':v, 'predictors':p})
idf.to_csv('idf_rf.csv')
```

```{python}
importance_df = pd.read_csv('importance_rf_1.csv')
idf=pd.read_csv('idf_rf.csv')
(
  idf 
    # >> filter(_.predictors<150)
    >> pivot_longer(
        cols=['MSE','R2_adj' ],names_to='variables', values_to='value'
                  )
    >> ggplot(aes(x ='predictors', y='value'))
    +  geom_point(alpha = 0.5)
    +  facet_wrap('~variables', ncol=1, scales ='free_y')
)


(
  idf 
    >> filter(_.predictors<80)
    >> pivot_longer(
        cols=['MSE','R2_adj' ],names_to='variables', values_to='value'
                  )
    >> ggplot(aes(x ='predictors', y='value'))
    +  geom_point(alpha = 0.5)
    +  geom_smooth(span=.3)
    +  facet_wrap('~variables', ncol=1, scales ='free_y')
)
```


```{python}

pd.options.display.float_format = '{:.4f}'.format

(
  idf 
            >> select(-_.vars) 
            >> mutate (MSE = _.MSE.round(2), R2_adj = _.R2_adj.round(3))
            >> top_n(10, _.R2_adj)
            >> arrange(_.R2_adj, -_.MSE)
)
```
```{python}
#| label: bm_f_rf
bm_features = ['Neighborhood_Stone_Brook', 'Land_Contour_Low', 'Lot_Shape_Regular', 'BsmtFin_Type_1_Unf', 'Neighborhood_Northridge_Heights', 'Overall_Cond_Excellent', 'BsmtFin_Type_1_No_Basement', 'Paved_Drive_Paved', 'Garage_Type_Detchd', 'Bldg_Type_OneFam', 'House_Style_Two_Story', 'Garage_Finish_Unf', 'Garage_Cond_Typical', 'Heating_QC_Typical', 'Bsmt_Exposure_No', 'Kitchen_AbvGr', 'MS_Zoning_Residential_Low_Density', 'Central_Air_Y', 'Bedroom_AbvGr', 'MS_SubClass_Two_Story_1946_and_Newer', 'Half_Bath', 'Neighborhood_Crawford', 'Bsmt_Unf_SF', 'Foundation_CBlock', 'BsmtFin_SF_1', 'Bsmt_Exposure_Gd', 'Longitude', 'Roof_Style_Hip', 'Lot_Frontage', 'Wood_Deck_SF', 'BsmtFin_Type_1_GLQ', 'Bsmt_Full_Bath', 'Roof_Style_Gable', 'Full_Bath', 'Open_Porch_SF', 'Second_Flr_SF', 'Foundation_PConc', 'Mas_Vnr_Area', 'TotRms_AbvGrd', 'Fireplaces', 'Year_Remod_Add', 'Lot_Area', 'Latitude', 'Garage_Area', 'Year_Built', 'Garage_Cars', 'First_Flr_SF', 'last_remod', 'Total_Bsmt_SF', 'Gr_Liv_Area']


# (
#   idf       >> filter(_.predictors ==50 )
#             >> pull('vars')
# )[0]

```


```{python}
selector = ColumnTransformer(
  [('select', 'passthrough', bm_features)],
  verbose_feature_names_out= False,
  remainder = 'drop'
).set_output(transform='pandas')


rf_model = RandomForestRegressor(
                  n_estimators = 350,
                  n_jobs=7, 
                  max_depth=13, 
                  max_features=7, 
                  min_samples_leaf=3,
                  min_samples_split=6, 
                  random_state=8)


rf_pipe = Pipeline([
  ('preprocesado', prep),
  ('selector', selector),
  ('rf', rf_model)
])


rf_pipe.fit(ames_x_train, Sale_Price_train)
importances_df = pd.DataFrame({"Variable" : rf_pipe.named_steps['rf'].feature_names_in_ , 
                               "Importancia" : rf_pipe.named_steps['rf'].feature_importances_})

```

```{python}
(
  importances_df >>
  ggplot(aes(x = 'reorder(Variable, Importancia)', y = "Importancia")) +
  geom_point(alpha = 0.65) +
  coord_flip()
)
```

```{python}
y_pred = rf_pipe.predict(ames_x_test)

get_metrics(y_pred, Sale_Price_test, predictors=50)
```


```{python}
y_pred = rf_pipe.predict(ames_x_val)

get_metrics(y_pred, Sale_Price_validation, predictors=50)
```
 
 