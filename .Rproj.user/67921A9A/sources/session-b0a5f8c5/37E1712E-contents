---
title: "EDA"                    
author: "Uriel Alexis Luna López"
format: html                    
editor: visual                  
---

# Analisís de las variables

## Descripción de las variables

### Variables continuas y númericas

| Variable                    | Descrición                                                                           |
|----------------------|:-------------------------------------------------|
| Lot_Frontage (Continua)     | Loguitud que conectan la calle con la propiedad (en pies)                            |
| Lot Area (Continua)         | Tamaño del lote en pies cuadrados                                                    |
| Mas Vnr Area (Continua)     | Área de revestimiento de mampostería en pies cuadrados                               |
| BsmtFin SF 1 (Continua)     | Area del sótano terminada Tipo 1 en pies cuadrados                                   |
| BsmtFin SF 2 (Continua)     | Area del sótano terminada Tipo 2 en pies cuadrados                                   |
| Bsmt Unf SF (Continua)      | Área sin terminar del sótano en pies cuadrados                                       |
| Total Bsmt SF (Continua)    | Área total del sótano                                                                |
| 1st Flr SF (Continua)       | Pies cuadrqados del primer piso                                                      |
| 2nd Flr SF (Continua)       | Pies cuadrados del segundo piso                                                      |
| Low Qual Area SF (Continua) | Pies cuadrados terminados de baja calidad                                            |
| Gr Liv Area (Continua)      | Superficie habitable sobre el nivel del suelo en pies cuadrados                      |
| Wood Deck SF (Continua)     | Área de cubierta de madera en pies cuadrados                                         |
| Open Porch SF (Continua)    | Área abierta del porche en pies cuadrados                                            |
| Enclose Porch (Continua)    | Área cerrada del proche en pies cuadrados                                            |
| 3-Ssn Porch (Continua)      | Área de porche de tres estaciones en pies cuadrados                                  |
| Screen Porch (Continua)     | Área del porche cubierto en pies cuadrados                                           |
| Pool Area (Continua)        | Área de la piscina en pies cuadrados                                                 |
| Misc Val (Continua)         | Valor de la característica miscelanea                                                |
| **Sale_Price (Continua)**   | Precio de venta.                                                                     |
| Year Built (Discreta)       | Año original de construcción                                                         |
| Year Remod/Add (Discreta)   | Fecha de remodelación (la misma que la fecha de construcción si no se ha remodelado) |
| Bsmt Full Bath (Discreta)   | Baños completos en el sótano                                                         |
| Bsmt Half Bath (Discreta)   | Medios baños en el sótano                                                            |
| Full Bath (Discreta)        | Baños completos sobre el nivel del suelo                                             |
| Half Bath (Discreta)        | Medios baños sobre el nivel del suelo                                                |
| Kitchen (Discreta)          | Concinas sobre el nivel del suelo                                                    |
| TotRmsAbvGrd                | Total de habitaciones sobre el nivel del suelo (no incluye baños)                    |
| Fireplaces (Discreta)       | Número de chimeneas                                                                  |
| Garage Yr Blt (Discreta)    | Año en que se construyó el garage                                                    |
| Garage Cars (Discreta)      | Tamaño del garage en cuanto a capacidad para autos.                                  |
| Mo Sold (Discreta)          | Mes de la venta (MM)                                                                 |
| Yr Sold (Discreta)          | Año de la venta (YYYY)                                                               |

### Varaibles categóricas

| Variable                 | Descrición                                                                             |
|---------------------|---------------------------------------------------|
| MS_Subclass (Nominal)    | Identifica el tipo de vivienda de la venta                                             |
| MS_Zoning (Nominal)      | Identifica la clasificación de la zona asociada a la venta                             |
| Street (Nominal)         | Tipo de camino de acceso a la propiedad                                                |
| Alley (Nominal)          | Tipo de callejón de acceso a la propiedad                                              |
| Lot Contour (Nominal)    | Planitud de la propiedad                                                               |
| Lot Config (Nominal)     | Configurarición del lote                                                               |
| Neightboorhood (Nominal) | Ubicación física dentro de los límites de la ciudad de Ames                            |
| Condition 1 (Nominal)    | Proximidad a varias condiciones                                                        |
| Condition 2 (Nominal)    | Proximidad a varias condiciones (Si más de una es presente)                            |
| Bldg Type (Nominal)      | Tipo de vivienda                                                                       |
| House Style (Nominal)    | Estilo de la vivienda                                                                  |
| Roof Style (Nominal)     | Tipo de techo                                                                          |
| Roof Matl (Nominal)      | Material del techo                                                                     |
| Exterior 1 (Nominal)     | Revestimiento exterior de la casa                                                      |
| Exterior 2 (Nominal)     | Revestimiento exterior de la casa (Si hay más de un material)                          |
| Mas Vnr Type (Nominal)   | Tipo de revestimiento de mampostería                                                   |
| Foundation (Nominal)     | Tipo de cimentación                                                                    |
| Heating (Nominal)        | Tipo de calefacción                                                                    |
| Central Air (Nominal)    | Aire acondicionado centralizado                                                        |
| Garage Type (Nominal)    | Ubicación del garage                                                                   |
| Misc Feature (Nominal)   | Característica miscelánea no cubierta en otras categorías                              |
| Sale Type (Nominal)      | Tipo de venta                                                                          |
| Sale Condition (Nominal) | Condicón de la venta.                                                                  |
| Functional (Ordinal)     | Funcionalidad del hogar (suponga que es típico a menos que se justifiquen deducciones) |
| FireplaceQu (Ordinal)    | Calidad de las chimeneas                                                               |
| Lot Shape (Ordinal)      | Forma general de la propiedad                                                          |
| Pool QC (Ordinal)        | Calidad de la piscina                                                                  |
| Utilities (Ordinal)      | Tipo de utilidades disponibles                                                         |
| Land Slope (Ordinal)     | Pendiente de la propiedad                                                              |
| Overall Qual (Ordinal)   | Califica el material general y el acabado de la casa.                                  |
| Overall Cond (Ordinal)   | Califica la condición general de la casa                                               |
| Exter Qual (Ordinal)     | Evalua la calidad materiales de los exteriores                                         |
| Exter Cond (Ordinal)     | Evalua la condición presente del materia en el exterior                                |
| Bsmt Qua (Ordinal)       | Evalua la altura del sótano                                                            |
| Bsmt Cond (Ordinal)      | Evalua la condición general del sótano                                                 |
| Bsmt Exposure (Ordinal)  | Se refiere a paredes a nivel de jardín o de salida                                     |
| BsmtFin Type 1 (Ordinal) | Calificación del area terminada del sótano                                             |
| BsmtFin Type 2 (Ordinal) | Clasificación del área terminada del sótano (si hay varios tipos)                      |
| Heating QC (Ordinal)     | Calidad y condición de la calefacción                                                  |
| Electrical (Ordinal)     | Sistema electrico                                                                      |
| Kitchen Qaul (Ordinal)   | Calidad de la cocina                                                                   |
| Garage Qual (Ordinal)    | Calidad del Garage                                                                     |
| Garage cond (Ordinal)    | Condicón del garage                                                                    |
| Paved Drive (Ordinal)    | Camino pavimentado.                                                                    |
| Garage Finish (Ordnal)   | El interior del garage esta terminado                                                  |
| Fence (Ordinal)          | Calidad de la cerca.                                                                   |

```{r}
#| label: startPy
#| include: false
library(reticulate)

use_virtualenv("CD_AMAT_2023")
```

```{python}
#| label: load-py-packages
#| include: false

import pandas as pd
from siuba import *
import numpy as np
from plotnine import *
import plydata as pr
from plydata.tidy import pivot_wider, pivot_longer
import matplotlib.pyplot as plt
```

## Analísis de Sale Price

A continuación se muestra el split inicial

```{python}
#| label: intial_split
#| echo: true

from sklearn.model_selection import train_test_split
from plydata.one_table_verbs import pull
from mizani.formatters import comma_format, dollar_format




ames = pd.read_csv("../data/ames.csv") >> mutate (Price_4_GLA= _.Sale_Price/_.Gr_Liv_Area)

y = ames >>  pull("Price_4_GLA")
X = select(ames, -_.Sale_Price, -_.Price_4_GLA)

X=(X >> mutate(wood_prop= _.Wood_Deck_SF/_.Gr_Liv_Area,
              area_per_car=case_when({
                              _.Garage_Cars == 0:0,
                              True:_.Garage_Area/ _.Garage_Cars}),
              last_remod = case_when({
                              _.Year_Built>_.Year_Remod_Add: 2023-_.Year_Remod_Add,
                              True: 2023-_.Year_Built}),
              Mo_Sold= _.Mo_Sold.astype('object')))

numeric_column = ames >> pull("Price_4_GLA")
quartiles = np.percentile(numeric_column, [25, 50, 75])

# Crea una nueva variable categórica basada en los cuartiles
stratify_variable = pd.cut(
 numeric_column, 
 bins=[float('-inf'), quartiles[0], quartiles[1], quartiles[2], float('inf')],
 labels=["Q1", "Q2", "Q3", "Q4"]
 )

ames_x_train, ames_x, ames_y_train, ames_y = train_test_split(
                       X, y, 
                       test_size = 0.30, 
                       random_state = 12345, 
                       stratify = stratify_variable
                       )

stratify_variable = pd.cut(
 ames_y , 
 bins=[float('-inf'), quartiles[0], quartiles[1], quartiles[2], float('inf')],
 labels=["Q1", "Q2", "Q3", "Q4"]
 )

ames_x_test, ames_x_val, ames_y_test, ames_y_val = train_test_split(
       ames_x, ames_y, 
       test_size = 1/3, 
       random_state = 12345, 
       stratify = stratify_variable
       )
       
Sale_Price_train = ames_y_train * ames_x_train.Gr_Liv_Area
Sale_Price_test = ames_y_test * ames_x_test.Gr_Liv_Area
Sale_Price_validation = ames_y_val * ames_x_val.Gr_Liv_Area
print('Totales:\t', ames.shape[0],'\nTraining:\t', ames_y_train.shape[0],'\nTesting:\t', ames_y_test.shape[0],'\nValidation:\t', ames_y_val.shape[0])
 
```

Cómo se observa, la distribución de los precios de venta se encuentra sesgada a la derecha, tienendo el 75% de las casas vendidas a un precio menor a \$212,075 y aclumulando un la mitad de estos entre \$129K y 212K. También en el siguiente qqplot podemos ver que la distribución de los precios de venta no parece seguir una distribución normal.

```{python}
#| output: false
#| echo: false
Sale_Price=ames_y_train*ames_x_train.Gr_Liv_Area

Sale_Price.describe()
summary = "count: 2051\nmean: $179,122.64\nstd: $79,337.45\nQ1: $128,225\nQ2:$159,500\nQ3: $211,000\nmin: $12,789\nmax:$755,000"
```

```{python}
#| results: hide
#| code-fold: true
#| label: sale_price_dist
#| fig-keep: 'all'

# plt.figure()
(
  ggplot(aes(y=Sale_Price, x=0))+
                geom_violin( fill='#702C27',
                                style='right',
                                alpha=0.6)+
                geom_jitter(aes(x=np.zeros(2051)-0.21),
                            size=1,
                            alpha=0.4,
                            color='#006DC8',
                            width=0.2,
                            random_state=7)+
                geom_boxplot(aes(x=np.zeros(2051)-0.21),width=0.4,
                                    color='#182E40',
                                    alpha=0.1,
                                    outlier_alpha=1,
                                    outlier_color='#235C6C',
                                    outlier_size=1.5)+
                coord_flip()+
                theme_classic()+
                ggtitle('Distribución de los precios de venta')+
                scale_y_continuous(labels=dollar_format(big_mark=','))+
                ylab('Precio de venta')+
                annotate('text',x=0.3,y=500000, label=summary,ymax=400000)+
                geom_text(ha='left',label=summary, x=0.3, y=500000, size=10, fontweight=1)+
                theme(axis_ticks_minor_y=element_blank(),
                        axis_ticks_major_y=element_blank(),
                        axis_text_y=element_blank()
                        )+
                theme(figure_size=(8, 6))
)

```

```{python}
#| label: qqplot_Sale_Price
#| echo: false
#| results: hide

params={'loc':Sale_Price.mean(), 'scale':Sale_Price.std()}

(
                ggplot(aes(sample=Sale_Price))+
                geom_qq(alpha = 0.3,distribution='norm', dparams=params)+
                stat_qq_line(distribution='norm', dparams=params,color = "red")+
                scale_y_continuous(labels=dollar_format(big_mark=',', digits = 0)) + 
                scale_x_continuous(labels=dollar_format(big_mark=',', digits = 0)) + 
                xlab("Cuantilies de la dist. normal teórica") + ylab("Cuantiles de Sale Price") +
                ggtitle("QQ-Plot N~(mu=180,996.79, sd=81,186.69)")+
                theme_seaborn()
            
)
```

A diferencia de la distribución de los precios se venta de la casa, el precio por pie cuadrado habitable de la casa parece tener un comportamiento más normal:

```{python}
#| results: hide
#| code-fold: true
#| label: sale_price_GLA_DIST
#| fig-keep: 'all'
summary = "count: 2344\nmean: $180,996.79\nstd: $81,186.69\nQ1: $129,500\nQ2:$160,000\nQ3: $213,492.50\nmin: $12,789\nmax:$755,000"
# plt.figure()
(
  ggplot(aes(y=ames_y_train, x=0))+
                geom_violin( fill='#702C27',
                                style='right',
                                alpha=0.6)+
                geom_jitter(aes(x=np.zeros(2051)-0.21),
                            size=1,
                            alpha=0.4,
                            color='#006DC8',
                            width=0.2,
                            random_state=7)+
                geom_boxplot(aes(x=np.zeros(2051)-0.21),width=0.4,
                                    color='#182E40',
                                    alpha=0.1,
                                    outlier_alpha=1,
                                    outlier_color='#235C6C',
                                    outlier_size=1.5)+
                coord_flip()+
                theme_classic()+
                ggtitle('Distribución de los precios de venta')+
                scale_y_continuous(labels=dollar_format(big_mark=','))+
                ylab('Precio de venta')+
                # annotate('text',x=0.3,y=500000, label=summary,ymax=400000)+
                geom_text(ha='left',label=summary, x=0.3, y=500000, size=10, fontweight=1)+
                theme(axis_ticks_minor_y=element_blank(),
                        axis_ticks_major_y=element_blank(),
                        axis_text_y=element_blank()
                        )+
                theme(figure_size=(8, 6))
)

```

```{python}
#| results: hide
#| code-fold: true
#| label: qq plot new y
#| fig-keep: 'all'
params={'loc':ames_y_train.mean(), 'scale':ames_y_train.std()}

(
                ggplot(aes(sample=ames_y_train))+
                geom_qq(alpha = 0.3,distribution='norm', dparams=params)+
                stat_qq_line(distribution='norm', dparams=params,color = "red")+
                scale_y_continuous(labels=dollar_format(big_mark=',', digits = 0)) + 
                scale_x_continuous(labels=dollar_format(big_mark=',', digits = 0)) + 
                xlab("Cuantilies de la dist. normal teórica") + ylab("Cuantiles de Sale Price") +
                ggtitle("QQ-Plot N~(mean=180,996.79, sd=81,186.69)")+
                theme_seaborn()
            
)
```

## Variables Continuas VS Transformación de Sale_Price

```{python}
#| echo: false
continuas = [   'Lot_Frontage',
                'Lot_Area', 
                'Mas_Vnr_Area', 
                'Sale_Price/Gr_Liv_Area',
                'BsmtFin_SF_1', 
                'BsmtFin_SF_2', 
                'Bsmt_Unf_SF', 
                'Total_Bsmt_SF', 
                'First_Flr_SF', 
                'Second_Flr_SF', 
                # 'Low_Qual_Area_SF', 
                'Gr_Liv_Area', 
                'Wood_Deck_SF', 
                'Open_Porch_SF', 
                'Enclosed_Porch', 
                'Three_season_porch', 
                'Screen_Porch', 
                'Pool_Area', 
                'Misc_Val']

```

```{python}
#| label: corr_heatmap_HCC
#| results: hide
#| code-fold: true
#| fig-keep: 'all'

ames_train = ames_x_train.copy()
ames_train['Sale_Price/Gr_Liv_Area'] = ames_y_train
correlation_data = (
    ames_train[continuas].corr().melt(  ignore_index=False,
                                        var_name='var1').reset_index() >>
                    select( _.var1,
                            _.var2 == _.index,
                            _.cor == _.value  ) 
)


labels=(
    correlation_data >> case_when({_.cor.between(-0.3, 0.3):'',
                                    _.cor>0.99991: '',
                                    True :_.cor.round(2).astype(str)})
)

(
    correlation_data >>
        arrange(-_.cor,_.var1) >>
        # filter (_.var1!=_.var2) >>
        mutate(labb=labels)>>
        ggplot(aes(x='var1', y='var2', fill= 'cor'))+
        geom_tile()+
        geom_label(aes(label='labb'), size=6)+
        scale_fill_gradientn(colors=['#0092ff','#e6d6c7','#FF1400'],
                            limits=(-1,1),
                            name='Correlación')+
        ggtitle('Mapa de calor de correlaciones para variables continuas')+
        xlab('')+ylab('')+#guides(fill=guide_legend(title="New Legend Title"))+
        theme(axis_text_x=element_text(rotation=90,hjust = -1),
                figure_size=(8,6))
)


```

Como podemos observar para ser que las varaibles de `'BsmtFin_SF_1', 'First_Flr_SF', 'Second_Flr_SF', 'Total_Bsmt_SF'` son las que tienen correlaciones más grandes con el precio de venta por el precio por pie cuadrado habitable. En el gráfico sólo se muestran etiquetas de correlaciones que no estan en el intervalo (-0.3.0.3).

```{python}
#| label: corr_sp/gla
#| results: hide
#| code-fold: true
#| fig-keep: 'all'
(
    ames_train[continuas].corr().reset_index(drop=False) 
    >> select(_['Sale_Price/Gr_Liv_Area'],_.index)
    >> filter(_['Sale_Price/Gr_Liv_Area'] != 1)
    >> mutate (labels = np.round(_['Sale_Price/Gr_Liv_Area'], 3))
    >> arrange(_['Sale_Price/Gr_Liv_Area'])
    >> ggplot(aes(y='Sale_Price/Gr_Liv_Area', x='index', fill='Sale_Price/Gr_Liv_Area'))
    + geom_col()
    + geom_text(aes(label = 'labels') , size=8, va='center', angle=90)
    + ylim([-0.5,0.5])
    + labs(title = 'Sale_Price/Gr_Liv_Area vs otras variables', y='', x='')
    + scale_fill_gradient2(low='#4EB043',
                           mid='#B05F43',
                           high= '#3A3AD5',
                           midpoint=0,
                           limits=(-1,1),
                           name='Correlación')
    + theme_bw()
    + theme(axis_text_x=element_text(angle=90))
)
```

## Precios dependiendo la ubicación

```{python}
#| label: price por ubi
#| results: hide
#| code-fold: true
#| fig-keep: 'all'
colors=['#EC00FF' ,'#0028F1','#00F104','#FF2D00']
(ames_train >> ggplot(aes(x='Latitude', y='Longitude', color='Sale_Price/Gr_Liv_Area')) +
                geom_point(alpha=0.5, size=1.5, stroke=1, shape='2')+
                labs(title='Precios dependiendo de la ubicación geografica', 
                    colour='Precio de venta', 
                    y='Longuitud',
                    x= 'Latidud')+
                scale_color_gradientn(colors=colors,labels=dollar_format(big_mark=','))+ theme_538()
 )


```

```{python}
#| label: error_bars por vecindario
#| results: hide
#| code-fold: true
#| fig-keep: 'all'
(
  ames_train 
        >> group_by(_.Neighborhood) 
        >> summarize(media = np.mean(_['Sale_Price/Gr_Liv_Area']), desv=np.std(_['Sale_Price/Gr_Liv_Area']))
        >> mutate( max_err = _.media  + _.desv, min_err =  _.media  - _.desv)
        >> arrange(_.media)
        >> ggplot(aes(x='reorder(Neighborhood,media)', y='media') ) 
        + geom_errorbar(aes(ymin='min_err', ymax='max_err'),width=0.3, position=position_dodge(0.9))+geom_point()
        + theme_bw()
        + theme(axis_text_x=element_text(angle=90))
        + scale_y_continuous(labels=dollar_format(big_mark=',', digits = 0)) 
        +coord_flip()
        +labs(y='Precio  promedio por pie cuadrado de área habitable', x='')
)
```

## Variables relacionadas con el sótano

Se puede ver que se tiene una relación líneal creciente entre `Total_Bsmt_SF` y `Sale_Price\ Gr_Liv_Area`, en la siguiente gráfica tambien podemos ver como varios puntos se acumulan en `Total_Bsmt_SF = 0` debido a que estas casas no cuentan con sótano

```{python}
#| label: price vs sf sotano
#| results: hide
#| code-fold: true
#| fig-keep: 'all'
(
    ames_x_train 
        >> ggplot( aes(y=ames_y_train, x= 'Total_Bsmt_SF'))
        + geom_point(size=1,alpha=0.4)
        + geom_smooth(color='red')
        + scale_y_continuous(labels = dollar_format(digits=0, big_mark=',') )
        +xlim([0,2500])
        + labs(title = 'Total_Bsmt_SF vs  Sale Price/ Gr_Liv_Area', x='Total_Bsmt_SF', y='Sale Price/ Gr_Liv_Area')
        + theme_538()
)
```

En cuanto a las variables categóricas relacionadas al sótano encontramos lo siguiente:

```{python}
#| label: analisis sotano
#| results: hide
#| code-fold: true
#| fig-keep: 'all'
cat_columns = pd.Series(['Bldg_Type', 
            'Bsmt_Exposure', 
            'Central_Air', 
            'Condition_1', 
            'Condition_2', 
            'Electrical', 
            'Fence', 
            'Foundation', 
            'Garage_Finish', 
            'Garage_Type', 
            'Misc_Feature', 
            'Paved_Drive', 
            'Heating_QC',
            'Overall_Cond',
            'MS_SubClass', 
            'MS_Zoning',
            'Bsmt_Cond',
            'BsmtFin_Type_1', 
            'BsmtFin_Type_2',
            'House_Style',
            'Lot_Shape',
            'Neighborhood'])
for category in cat_columns[cat_columns.str.contains('Bsmt')]:
  titulo='Categoría analazada: {}'.format(category) 
  (ames_train  >> ggplot(aes(y= 'Sale_Price/Gr_Liv_Area', x= 'Total_Bsmt_SF', color= '{}'.format(category)))
              + geom_point(alpha= 0.5, show_legend=False) + geom_smooth(color='black')
              +xlim([0,3000])
              +labs(x='Área total del sótano', y='Precio por pie cuadrado de área habitable bruta', title=titulo)
              + facet_wrap('~{}'.format(category))+theme_538 ()) 
```

Puntos a considerar:

-   En `Bsmt_Exposure` se puede notar que la relación positiva entre las variables se mantiene, y no parece cambiar si se altera la exposicón del sótano. Además se muestran elementos con `Total_Bmst_SF`\>0 que caen en la categoría de No basement, lo cual puede deberse a un error.

-   En `Bsmt_Cond` se puede ver que la relación de las variables si cambia dependiendo de la condición del sótano, además en la condición de Excellent y Poor se tienen muy pocas observaciones.

-   En `Bsmt_Type_1` se tienen relaciones demaciado planas en su mayoria, a excepción de GLQ y Unf.

-   En `Bsmt_Type_2` se tiene el mismo caso que en el anterior. Además se muestran elementos con `Total_Bmst_SF`\>0 que caen en la categoría de No basement, lo cual puede deberse a un error.

# Regresión Líneal

```{python}
#| label: libraries lm
#| echo: false

from mlxtend.feature_selection import ColumnSelector
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder,FunctionTransformer
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate
import statsmodels.api as sm 
```

```{python}
#| label: funciones creadas lm
#| results: hide
#| code-fold: true
pd.options.display.float_format = '{:.2f}'.format

def importance_from_model (test_frame, y_obs, selected_columns, pipeline, actual_mse, n_permutations=50 ,trans_pred= False ):
  performance_losses = []
  
  for i in range(test_frame[selected_columns].shape[1]):
    loss = []
    for j in range(n_permutations):
        test_frame_permuted = test_frame[selected_columns].copy()
        test_frame_permuted.iloc[:, i] = np.random.permutation(test_frame_permuted.iloc[:, i])
        if trans_pred == False:
            y_pred_permuted = pipeline.predict(test_frame_permuted)
        else:
            y_pred_permuted = pipeline.predict(test_frame_permuted)* test_frame_permuted.Gr_Liv_Area
        mse_permuted = mean_squared_error(y_obs, y_pred_permuted)
        loss.append(mse_permuted)
    performance_losses.append(loss)
  
  performance_losses = performance_losses/np.sum(performance_losses, axis=0)
  mean_losses = np.mean(performance_losses, axis=1)
  std_losses = np.std(performance_losses, axis=1)
  
  importance_df = pd.DataFrame({
  'Variable': selected_columns, 
  'Mean_Loss': mean_losses, 
  'Std_Loss': std_losses
  })
  return importance_df
  
def adjusted_r2_score(y_true, y_pred, n, p):
  r2 = r2_score(y_true, y_pred)
  adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
  return adjusted_r2 


def div_columns(X, c1, c2):
    X["c1_c2"] = X[c1]/ X[c2]
    return X



def adj_r(y_o, y_p , p, n):
  r2_adj = 1 - (n - 1) / (n - p - 1) * (1 - r2_score(y_o, y_p))
  
  
def get_metrics (y_pred, y_obs,predictors):
  me = np.mean(y_obs - y_pred)
  mae = mean_absolute_error(y_obs, y_pred)
  mape = mean_absolute_percentage_error(y_obs, y_pred)
  mse = mean_squared_error(y_obs, y_pred)
  rmse = np.sqrt(mse)
  r2 = r2_score(y_obs, y_pred)

  n = len(y_obs)  # Número de observaciones
  p = predictors  # Número de predictores 
  r2_adj = 1 - (n - 1) / (n - p - 1) * (1 - r2)
  

  metrics_data = {
      "Metric": ["ME", "MAE", "MAPE", "MSE", "RMSE", "R2", "R2Adj"],
      "Value": [me, mae, mape, mse, rmse, r2, r2_adj]
  }
  return pd.DataFrame(metrics_data).set_index('Metric')



def rmv_elements (list_of_elements, complete_list):
  for element in list_of_elements:
    complete_list.remove(element)
  return complete_list



def validation_results(x_val,y_val,  pipeline , n_preds):
  y_pred = pipeline.predict(x_val)
  
  ames_test = (
    ames_x_val >>
    mutate(Sale_Price_Pred = y_pred*_.Gr_Liv_Area, Sale_Price =y_val*_.Gr_Liv_Area))
  
  ##### Métricas de desempeño
  
  y_obs = ames_test["Sale_Price"]
  y_pred = ames_test["Sale_Price_Pred"]
  return get_metrics(y_pred, y_obs, n_preds)


```

A continuación se muestra el modelo de regresión líneal que tuvo un mejor performance de acuerdo a las variables que se estudiaron:

```{python}
#| label: resultaqdos lm test
#| code-fold: true
num_cols =sorted( ['Wood_Deck_SF', 
            'First_Flr_SF',
            'Fireplaces',
            'Open_Porch_SF',
            'Enclosed_Porch',
            'Lot_Area',
            'wood_prop',
            'Total_Bsmt_SF',
            'Garage_Area',
            'Gr_Liv_Area',
            'area_per_car',
            'last_remod',
            'Bsmt_Full_Bath', 
            'Three_season_porch',
            'BsmtFin_SF_1', 
            'BsmtFin_SF_2', 
            'Bsmt_Unf_SF',
            'Bsmt_Half_Bath', 
            'Full_Bath',
            'Kitchen_AbvGr',
            'Half_Bath',
            'Mas_Vnr_Area',
            'Misc_Val',
            'Lot_Frontage',
            'Bedroom_AbvGr'])

cat_cols = ['Bldg_Type', 
            'Bsmt_Exposure', 
            'Central_Air','Mo_Sold', 
            'Condition_1', 
            'Condition_2', 
            'Electrical', 
            'Mas_Vnr_Type',
            'Fence', 
            'Exter_Cond',
            'Foundation', 
            'Garage_Finish', 
            'Garage_Type', 
            # 'Misc_Feature', 
            'Paved_Drive', 
            'Heating_QC',
            'Overall_Cond',
            'MS_SubClass', 
            'Bsmt_Cond',
            'BsmtFin_Type_1', 
            'BsmtFin_Type_2', 
            'House_Style',
            'Lot_Shape', #'Roof_Style'
            'Neighborhood']
            
columnas_seleccionadas = num_cols + sorted(cat_cols + ['Year_Built','Garage_Cars','Misc_Feature'])

pipe = ColumnSelector(columnas_seleccionadas)
ames_x_train_selected = pipe.fit_transform(ames_x_train)

ames_train_selected = pd.DataFrame(
  ames_x_train_selected, 
  columns = columnas_seleccionadas
  )

ames_test_selected = pd.DataFrame(
  pipe.transform(ames_x_test), 
  columns = columnas_seleccionadas
  )

ames_validation_selected = pd.DataFrame(
  pipe.transform(ames_x_val), 
  columns = columnas_seleccionadas
  )
 
#######################################################################
preprocessor_1 = ColumnTransformer(
    transformers = [
        ('scaler', StandardScaler(), num_cols),
        ('imputer',  SimpleImputer(missing_values=np.nan, strategy='mean'), ['Year_Built','Garage_Cars']),
        ('OHE', OneHotEncoder(drop='first',handle_unknown='ignore' , sparse_output=False, min_frequency=20), cat_cols),
        ('OHE_1', OneHotEncoder(drop='first',handle_unknown='ignore' , sparse_output=False), ['Misc_Feature'])
    ],
    verbose_feature_names_out = False,
    remainder = 'passthrough'  # Mantener las columnas restantes sin cambios
    )


ames_x_train_trans = pd.DataFrame(
  preprocessor_1.fit_transform(ames_train_selected),
  columns=preprocessor_1.get_feature_names_out()
  )
  
ames_x_test_trans = pd.DataFrame(
  preprocessor_1.transform(ames_test_selected),
  columns=preprocessor_1.get_feature_names_out()
  )

ames_x_val_trans = pd.DataFrame(
  preprocessor_1.transform(ames_validation_selected),
  columns=preprocessor_1.get_feature_names_out()
  )
  
  
```

```{python}
from sklearn.preprocessing import PolynomialFeatures

interaction_transformer = PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)
interaction_transformer_wb = PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)
```

```{python}
drop_cols=['Neighborhood_Crawford',
            'Neighborhood_infrequent_sklearn',
            'BsmtFin_Type_1_BLQ',
            'Exter_Cond_Good',
            # 'Condition_2_Feedr',
            'Condition_1_RRAe',
            # 'Condition_1_Feedr',
            'Garage_Type_No_Garage',
            # 'Condition_1_PosA',
            'Bsmt_Full_Bath',
            'Bsmt_Half_Bath',
            # 'Condition_2_RRAe',
            'Foundation_CBlock',
            'MS_SubClass_Two_Story_1946_and_Newer',
            'Garage_Finish_No_Garage',
            'Bldg_Type_TwoFmCon',
            'Mo_Sold_5',
            'Paved_Drive_Partial_Pavement',
            'Bsmt_Cond_Typical',
            'Heating_QC_Fair',
            'BsmtFin_Type_1_Unf',
            'Electrical_SBrkr',
            'Fence_Good_Wood',
            # 'Electrical_Unknown',
            'Garage_Type_Detchd',
            'Foundation_Slab',
            'House_Style_infrequent_sklearn',
            'Three_season_porch',
            'BsmtFin_Type_2_GLQ',
            'Mo_Sold_7',
            'Misc_Val',
            'BsmtFin_Type_1_No_Basement',
            'Foundation_PConc',
            'Mo_Sold_12',
            'Mo_Sold_10',
            'BsmtFin_Type_2_No_Basement',
            'last_remod',
            'Bsmt_Cond_No_Basement',
            'Condition_2_infrequent_sklearn',
            'Exter_Cond_Typical',
            'Bldg_Type_OneFam',
            'Mo_Sold_9',
            'Electrical_infrequent_sklearn',
            'Bsmt_Cond_infrequent_sklearn',
            'Bsmt_Cond_Good',
            'Foundation_infrequent_sklearn',
            'Bldg_Type_TwnhsE',
            'Fence_No_Fence']

  

preprocessor_2=ColumnTransformer(
  transformers=[
    ("selector", "drop", drop_cols),
    ('interaction_1', interaction_transformer_wb, ['Lot_Area', 'Gr_Liv_Area']),
    ('interactions2', interaction_transformer, ['Year_Built', 'Overall_Cond_Average']),
    ('interactions3', interaction_transformer, ['Full_Bath', 'Bedroom_AbvGr']),
    ('interactions3.1', interaction_transformer, ['House_Style_SLvl', 'Overall_Cond_Fair']),
    ('interactions3.4', interaction_transformer, ['wood_prop', 'Mas_Vnr_Area']),
    ('interactions3.2', interaction_transformer, ['First_Flr_SF', 'last_remod']),
    ('interactions3.5', interaction_transformer, ['BsmtFin_SF_1', 'BsmtFin_Type_2_Unf']),
    ('interactions4', interaction_transformer, ['Misc_Val', 'Misc_Feature_TenC'])
  ],
  verbose_feature_names_out = False,
  remainder='passthrough'
)

preprocessor_2.fit(ames_x_train_trans)
feature_names = list(preprocessor_2.get_feature_names_out())

  
##### Extracción de coeficientes
transformed_df = pd.DataFrame(
  preprocessor_2.fit_transform(ames_x_train_trans ),
  columns=feature_names
  )
# 
X_train_with_intercept = sm.add_constant(transformed_df)
model = sm.OLS(ames_y_train, X_train_with_intercept).fit()

model.summary()

```

```{python}
pipeline = Pipeline([
  ('preprocessor', preprocessor_2),
  ('regressor', LinearRegression())])

# Entrenar el pipeline
results = pipeline.fit(ames_x_train_trans, ames_y_train)

## PREDICCIONES
y_pred = pipeline.predict(ames_x_test_trans)

ames_test = (
  ames_x_test >>
  mutate(Sale_Price_Pred = y_pred*_.Gr_Liv_Area, Sale_Price =ames_y_test*_.Gr_Liv_Area)
)

##### Métricas de desempeño

y_obs = ames_test["Sale_Price"]
y_pred = ames_test["Sale_Price_Pred"]

predictores = transformed_df.columns.to_list()
mtrcs_dt =get_metrics(y_pred, y_obs,predictors= len(transformed_df.columns ))


metrics_df = pd.DataFrame(mtrcs_dt)
print(metrics_df)

```

El modelo muestra una $R^2_{adj}=0.85$ con los datos de test, y tienen un error aproximado del $\pm10\%$ con respecto del precio real.

```{python}
validation_results(ames_x_val_trans, ames_y_val, pipeline, len(transformed_df.columns ))
```

En cuanto al análisis de los residuos, no parece seguir completamente los supuestos, pues visualmete se puede detectar heterocedasticidad, lo cual puede afectar el preformance del modelo.

```{python}
#### Gráficos de desempeño de modelo

(
    ggplot(aes(x = y_obs, y = y_pred)) +
    geom_point(alpha=0.5) +
    scale_y_continuous(labels = dollar_format(digits=0, big_mark=','), limits = [0, 600000] ) +
    scale_x_continuous(labels = dollar_format(digits=0, big_mark=','), limits = [0, 600000]) +
    geom_abline(color = "red") +
    coord_equal() +
    labs(
      title = "Comparación entre predicción y observación",
      x = "Predicción",
      y = "Observación")
)


(
ames_test >>
  select(_.Sale_Price, _.Sale_Price_Pred) >>
  mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
  ggplot(aes(x = "error")) +
  geom_histogram(color = "white", fill = "black") +
  geom_vline(xintercept = 0, color = "red") +
  scale_x_continuous(labels=dollar_format(big_mark=',', digits=0)) + 
  ylab("Conteos de clase") + xlab("Errores") +
  ggtitle("Distribución de error")
)


(
ames_test >>
  select(_.Sale_Price, _.Sale_Price_Pred) >>
  mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
  ggplot(aes(sample = "error")) +
  geom_qq(alpha = 0.3) + stat_qq_line(color = "red") +
  scale_y_continuous(labels=dollar_format(big_mark=',', digits = 0)) + 
  xlab("Distribución normal") + ylab("Distribución de errores") +
  ggtitle("QQ-Plot")
)


(
ames_test >>
  select(_.Sale_Price, _.Sale_Price_Pred) >>
  mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
  ggplot(aes(x = "Sale_Price")) +
  geom_linerange(aes(ymin = 0, ymax = "error"), colour = "purple") +
  geom_point(aes(y = "error"), size = 0.05, alpha = 0.5) +
  geom_abline(intercept = 0, slope = 0) +
  scale_x_continuous(labels=dollar_format(big_mark=',', digits=0)) + 
  scale_y_continuous(labels=dollar_format(big_mark=',', digits=0)) +
  xlab("Precio real") + ylab("Error de estimación") +
  ggtitle("Relación entre error y precio de venta")
)
```

# Regresión con Knn

```{python}
from mlxtend.feature_selection import ColumnSelector
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate
from sklearn.model_selection import GridSearchCV
from sklearn.utils import shuffle

from plydata.one_table_verbs import pull
from mizani.formatters import comma_format, dollar_format
from plotnine import *
from siuba import *

import pandas as pd
import numpy as np
from sklearn.ensemble import VotingRegressor
```


```{python}

cat_cols = ['MS_SubClass', 
                'MS_Zoning',
                'Street', 
                'Alley', 
                'Lot_Shape',
                'Land_Contour', 
                'Utilities', 
                'Lot_Config',
                'Land_Slope', 
                'Neighborhood',
                'Condition_1',
                'Condition_2', 
                'Bldg_Type', 
                'House_Style',
                'Overall_Cond', 
                'Roof_Style', 
                'Roof_Matl',
                'Exterior_1st',
                'Exterior_2nd', 
                'Mas_Vnr_Type', 
                'Exter_Cond',
                'Foundation', 
                'Bsmt_Cond',
                'Bsmt_Exposure', 
                'BsmtFin_Type_1', 
                'BsmtFin_Type_2', 
                'Heating',
                'Heating_QC', 
                'Central_Air', 
                'Electrical', 
                'Functional', 
                'Garage_Type',
                'Garage_Finish', 
                'Garage_Cond', 
                'Paved_Drive', 
                'Pool_QC', 
                'Fence',
                'Misc_Feature',
                'Mo_Sold', 
                # 'Sale_Type', 
                # 'Sale_Condition'
                ]

num_cols =[ 'Bedroom_AbvGr',
            'BsmtFin_SF_1',
            'BsmtFin_SF_2',
            'Bsmt_Full_Bath',
            'Bsmt_Half_Bath',
            'Bsmt_Unf_SF',
            'Enclosed_Porch',
            'Fireplaces',
            'First_Flr_SF',
            'Full_Bath',
            'Garage_Area',
            'Garage_Cars',
            'Gr_Liv_Area',
            'Half_Bath',
            'Kitchen_AbvGr',
            'Latitude',
            'Longitude',
            'Lot_Area',
            'Lot_Frontage',
            'Mas_Vnr_Area',
            'Misc_Val',
            'Open_Porch_SF',
            'Pool_Area',
            'Screen_Porch',
            'Second_Flr_SF',
            'Three_season_porch',
            'TotRms_AbvGrd',
            'Total_Bsmt_SF',
            'Wood_Deck_SF',
            'Year_Built',
            'Year_Remod_Add',
            # 'Year_Sold',
            'area_per_car',
            'last_remod',
            'wood_prop']  
```

## KNN usando unicamente variables categóricas

```{python}
preprocessor = ColumnTransformer(
    transformers = [
        ('onehotencoding', OneHotEncoder(drop='first',handle_unknown = 'ignore', sparse_output=False, min_frequency=15), cat_cols)
    ],
    verbose_feature_names_out = False,
    remainder = 'drop'  # Mantener las columnas restantes sin cambios
).set_output(transform = 'pandas')

transformed_df = preprocessor.fit_transform(ames_x_train) 

```

```{python}
kf = KFold(n_splits=10, shuffle=True, random_state=42)



scoring = {
    'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
    'r2': make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_x_train)), p=len(num_cols)),
    'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
    'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}


param_grid = {
 'n_neighbors': range(4, 20,),
 'weights': [ 'distance'],
 'metric': ['manhattan', 'hamming', 'cosine', 'chebyshev']
}


pipline_knn_cv=Pipeline([
  ('prep', preprocessor),
  ('cv' , GridSearchCV(
    KNeighborsRegressor(), 
      param_grid, 
      cv=kf, 
      scoring=scoring, 
      refit='neg_mean_squared_error',
      verbose=2, 
      n_jobs=7)
      )]
      )
```


```{python}
#| eval: false 
pipline_knn_cv.fit(ames_x_train, Sale_Price_train)
pd.to_pickle(pipline_knn_cv,'pipline_cateogrico_knn_cv.pkl')
```

```{python}
pipline_knn_cv = pd.read_pickle('pipline_cateogrico_knn_cv.pkl')
results = pipline_knn_cv.named_steps['cv'].cv_results_
best_knn_cat = pipline_knn_cv.named_steps['cv'].best_estimator_

summary_num_cv = (
  pd.DataFrame(results) >>
  select(-_.contains("split"), -_.contains("time"), -_.params)
)
(
  summary_num_cv 
  >> arrange(_.rank_test_r2) 
  >> select(_.contains('r2'), _.contains('param')) 
).head(10)
```

```{python}


summary_plot = (
  summary_num_cv 
  >> filter(_.mean_test_r2>0, _.param_n_neighbors>5) >>
  ggplot(aes(x = "param_n_neighbors", y = "mean_test_r2", 
             shape = "param_weights", color= 'param_metric')) +
  geom_point() +
  ggtitle("Parametrización de KNN vs R^2") +
  xlab("Parámetro: Número de vecinos cercanos") +
  ylab("R^2 promedio")
)


```

```{python}
results = pipline_knn_cv.named_steps['cv'].cv_results_
best_knn_cat = pipline_knn_cv.named_steps['cv'].best_estimator_

summary_num_cv = (
  pd.DataFrame(results) >>
  select(-_.contains("split"), -_.contains("time"), -_.params)
)
(
  summary_num_cv 
  >> arrange(_.rank_test_r2) 
  >> select(_.contains('r2'), _.contains('param')) 
).head(10)
```

```{python}
summary_plot = (
  summary_num_cv 
  >> filter(_.mean_test_r2>0, _.param_n_neighbors>4) >>
  mutate(min_err=_.mean_test_r2-_.std_test_r2,max_err=_.mean_test_r2+_.std_test_r2 ) >>
  ggplot(aes(x = "param_n_neighbors", y = "mean_test_r2", 
             shape = "param_weights", color= 'param_metric')) +
  geom_point(alpha = 1,position=position_dodge(width=0.7)) +
  geom_errorbar(aes(ymin='min_err', ymax='max_err'),
                  position=position_dodge(width=0.7),show_legend=False) +
  ggtitle("Parametrización de KNN vs R^2") +
  xlab("Parámetro: Número de vecinos cercanos") +
  ylab("R^2 promedio")
)

```


```{python}
knn_cat_best=KNeighborsRegressor(
           n_neighbors = 15,
           weights ='distance',
           metric='hamming')

knn_cat_best.fit(transformed_df, Sale_Price_train)

```

```{python}

test_transformed = preprocessor.transform(ames_x_test)
pred= knn_cat_best.predict(test_transformed)
predictors = len(preprocessor.get_feature_names_out())
mse= get_metrics(y_pred=pred, y_obs=Sale_Price_test, predictors=predictors) .loc['MSE']
```
```{python}
#| eval: false

importance = importance_from_model(
  test_transformed ,
  Sale_Price_test,
  list(preprocessor.get_feature_names_out()),
  knn_cat_best,
  actual_mse = mse,
  n_permutations = 50 ,
  trans_pred = False)
pd.to_pickle(importance,'importance_cat_knn.pkl')
```


```{python}
#| echo: false
importance = pd.read_pickle('importance_cat_knn.pkl')
importance >> arrange(_.Mean_Loss)
```

```{python}
(
  importance >> top_n(20, _.Mean_Loss) >>
  mutate(
    ymin = _.Mean_Loss - _.Std_Loss,
    ymax = _.Mean_Loss + _.Std_Loss) >>
  ggplot(aes(x = 'reorder(Variable, Mean_Loss)', y = "Mean_Loss")) +
  geom_errorbar(aes(ymin='ymin', ymax='ymax'),
    width=0.2, position=position_dodge(0.9)) +
  geom_point(alpha = 0.65) +
  labs(title='Importancia de las Variables', x='Variable', y='Importancia') +
  coord_flip()
)
```

```{python}
(
  importance >> top_n(-20, _.Mean_Loss)>>
  mutate(
    ymin = _.Mean_Loss - _.Std_Loss,
    ymax = _.Mean_Loss + _.Std_Loss) >>
  ggplot(aes(x = 'reorder(Variable, Mean_Loss)', y = "Mean_Loss")) +
  geom_errorbar(aes(ymin='ymin', ymax='ymax'),
    width=0.2, position=position_dodge(0.9)) +
  geom_point(alpha = 0.65) +
  labs(title='Importancia de las Variables', x='Variable', y='Importancia') +
  coord_flip()
)
```


```{python}
#| eval: false
vars_importance_order =list( importance >> arrange(_.Mean_Loss) >> pull('Variable'))

v=[]
r2=[]
mse_=[]
for i in range(150):
  vars_to_train = vars_importance_order[i:]
  
  knn_cat_best.fit(transformed_df[vars_to_train], Sale_Price_train)
  pred= knn_cat_best.predict(test_transformed[vars_to_train])
  mse_.append(get_metrics(pred,Sale_Price_test, len(vars_to_train)).loc['MSE'].values[0])
  r2.append(get_metrics(pred,Sale_Price_test, len(vars_to_train)).loc['R2Adj'].values[0])
  v.append(vars_to_train)
idf = pd.DataFrame({'MSE':mse_, 'R2_adj':r2,'vars':v})
best_features_from_cat = (idf >> top_n(-1, _.MSE) >> pull ('vars'))[0]
  
```
```{python}
#| echo: false

# pd.to_pickle(idf, 'bfcknn.pkl')
idf= pd.read_pickle('bfcknn.pkl')
best_features_from_cat = (idf >> top_n(-1, _.MSE) >> pull ('vars'))[0]
best_features_from_cat
```


```{python}
#| eval: false
selectorr= ColumnTransformer([('selector', 'passthrough', best_features_from_cat)],
          remainder= 'drop',
          verbose_feature_names_out=False)
          
param_grid = {
 'n_neighbors': range(4, 30),
 'weights': [ 'distance','uniform'],
 'metric': ['manhattan', 'hamming', 'cosine', 'chebyshev']
}


final_cat_knn_pipeline_cv = Pipeline([
  ('prep', preprocessor),
  ('selector', selectorr),
  ('cv', GridSearchCV(
    KNeighborsRegressor(), 
    param_grid, 
    cv=kf, 
    scoring=scoring, 
    refit='neg_mean_squared_error',
    verbose=2, 
    n_jobs=7))]
)

final_cat_knn_pipeline_cv.fit(ames_x_train, Sale_Price_train)
```
```{python}
#| echo: false
# pd.to_pickle(final_cat_knn_pipeline_cv,'cv_l_knn_cat.pkl')
selectorr= ColumnTransformer([('selector', 'passthrough', best_features_from_cat)],
          remainder= 'drop',
          verbose_feature_names_out=False)
final_cat_knn_pipeline_cv = pd.read_pickle('cv_l_knn_cat.pkl')
```

```{python}
(
  pd.DataFrame(final_cat_knn_pipeline_cv.named_steps['cv'].cv_results_) 
  >> filter(_.param_metric != 'chebyshev',
            _.param_weights == 'distance',
            _.param_metric != 'cosine',
            _.param_n_neighbors >12)
  >> mutate(min_err=_.mean_test_r2-_.std_test_r2,max_err=_.mean_test_r2+_.std_test_r2 )
  >> ggplot(aes(x = "param_n_neighbors", y = "mean_test_r2", 
             shape = "param_weights", color= 'param_metric')) +
  geom_point(alpha = 1,position=position_dodge(width=0.7)) +
  geom_errorbar(aes(ymin='min_err', ymax='max_err'),
                  position=position_dodge(width=0.7),show_legend=False) +
  ggtitle("Parametrización de KNN vs R^2") +
  ylim([0.6,0.8])+
  xlab("Parámetro: Número de vecinos cercanos") +
  ylab('R ^2 ajustada promedio')
)
```
```{python}
f_cat_knn_pipeline = Pipeline([
  ('prep', preprocessor),
  ('selector', selectorr),
  ('cv', KNeighborsRegressor(
            n_neighbors = 22,
            weights ='distance',
            metric='hamming'))]
)


f_cat_knn_pipeline.fit(ames_x_train, Sale_Price_train)

y_pred =  f_cat_knn_pipeline.predict(ames_x_test)
pd.options.display.float_format = '{:.4f}'.format
get_metrics(y_pred, Sale_Price_test, len(selectorr.get_feature_names_out()))
```

## KNN con variables númericas

```{python}
#| eval: false
preprocessor= ColumnTransformer([
  ('std', StandardScaler(), num_cols)
  ],
  remainder = 'drop', 
  verbose_feature_names_out= False).set_output(transform = 'pandas')

param_grid = {
 'n_neighbors': range(0, 105, 5),
 'weights': ['uniform', 'distance'],
 'metric': ['minkowski','euclidean', 'manhattan', 'hamming', 'cosine', 'chebyshev'],
 'p': range(3,10)
}
    
    
gs_knn_cv = Pipeline([
  ('prep', preprocessor),
  ('cv', GridSearchCV(
      KNeighborsRegressor(), 
      param_grid, 
      cv=kf, 
      scoring=scoring, 
      refit='neg_mean_squared_error',
      verbose=3, 
      n_jobs=7))
])

gs_knn_cv.fit(ames_x_train, Sale_Price_train)
```

```{python}
#| echo: false
# pd.to_pickle(gs_knn_cv, 'gs_knn_cv.pkl')
preprocessor= ColumnTransformer([
  ('std', StandardScaler(), num_cols)
  ],
  remainder = 'drop', 
  verbose_feature_names_out= False).set_output(transform = 'pandas')
gs_knn_cv = pd.read_pickle('gs_knn_cv.pkl')
```


```{python}

(
  pd.DataFrame(gs_knn_cv.named_steps['cv'].cv_results_) 
  # >> filter(_.param_metric != 'chebyshev',
  #           _.param_weights == 'distance',
  #           _.param_metric != 'cosine',
            # _.param_n_neighbors >12)
  # >> mutate(min_err=_.mean_test_r2-_.std_test_r2,max_err=_.mean_test_r2+_.std_test_r2 )
  >> ggplot(aes(x = "param_n_neighbors", y = "mean_test_r2", 
             shape = "param_weights", color= 'param_metric', size='param_p')) +
  geom_point(alpha = 1,position=position_dodge(width=0.7)) +
  # geom_errorbar(aes(ymin='min_err', ymax='max_err'),
                  # position=position_dodge(width=0.7),show_legend=False) +
  ggtitle("Parametrización de KNN vs R^2") +
  # ylim([0.6,0.8])+
  xlab("Parámetro: Número de vecinos cercanos") +
  ylab('R ^2 ajustada promedio')
)
```

```{python}
#| eval: false

param_grid = {
 'n_neighbors': range(5, 20),
 'weights': ['uniform', 'distance'],
 'metric': ['minkowski','euclidean', 'manhattan', 'hamming', 'cosine', 'chebyshev'],
 'p': range(3,10)
}
    
    
gs_knn_cv2 = Pipeline([
  ('prep', preprocessor),
  ('cv', GridSearchCV(
      KNeighborsRegressor(), 
      param_grid, 
      cv=kf, 
      scoring=scoring, 
      refit='neg_mean_squared_error',
      verbose=3, 
      n_jobs=7))
])

gs_knn_cv2.fit(ames_x_train, Sale_Price_train)
```

```{python}
#| echo: false

# pd.to_pickle(gs_knn_cv2,'gs_knn_cv2.pkl')gs_knn_cv2
gs_knn_cv2 = pd.read_pickle('gs_knn_cv2.pkl')
```

```{python}
# gs_knn_cv.named_steps['cv'].cv_results_

(
  pd.DataFrame(gs_knn_cv2.named_steps['cv'].cv_results_) 
  >> filter(_.param_metric == 'manhattan',
            _.param_weights == 'distance',_.param_n_neighbors>9
              )
  >> mutate(min_err=_.mean_test_r2-_.std_test_r2,max_err=_.mean_test_r2+_.std_test_r2 )
  >> ggplot(aes(x = "param_n_neighbors", y = "mean_test_r2", 
             shape = "param_weights", color= 'param_metric')) +
  geom_point(alpha = 1,position=position_dodge(width=0.7)) +
  geom_errorbar(aes(ymin='min_err', ymax='max_err'),
                  position=position_dodge(width=0.7),show_legend=False) +
  ggtitle("Parametrización de KNN vs R^2") +
  # ylim([0.6,0.8])+
  xlab("Parámetro: Número de vecinos cercanos") +
  ylab('R ^2 ajustada promedio')
)
```

```{python}
fst_knn_pipe = Pipeline([
  ('prep',preprocessor),
  ('regressor', KNeighborsRegressor(
    n_neighbors = 16,
    weights = 'distance',
    metric ='manhattan')
    )]
    )
    
fst_knn_pipe.fit(ames_x_train , Sale_Price_train)
y_pred  = fst_knn_pipe.predict(ames_x_test)

get_metrics(y_pred, Sale_Price_test, len(preprocessor.get_feature_names_out()))
get_metrics(y_pred, Sale_Price_test, len(preprocessor.get_feature_names_out()))
```

```{python}
#| eval: false
importance_df =importance_from_model(
  test_frame = ames_x_test,
  y_obs = Sale_Price_test ,
  selected_columns = num_cols, 
  pipeline = fst_knn_pipe,
  actual_mse =get_metrics(y_pred, Sale_Price_test, len(preprocessor.get_feature_names_out())).loc['MSE'].values[0] ,
  n_permutations= 50,
  trans_pred= False
)
```

```{python}
# pd.to_pickle(importance_df, 'knn_num_importance_df.pkl')
importance_df = pd.read_pickle('knn_num_importance_df.pkl')
```



```{python}
(
  importance_df >># top_n(-20, _.Mean_Loss)>>
  mutate(
    ymin = _.Mean_Loss - _.Std_Loss,
    ymax = _.Mean_Loss + _.Std_Loss) >>
  ggplot(aes(x = 'reorder(Variable, Mean_Loss)', y = "Mean_Loss")) +
  geom_errorbar(aes(ymin='ymin', ymax='ymax'),
    width=0.2, position=position_dodge(0.9)) +
  geom_point(alpha = 0.65) +
  labs(title='Importancia de las Variables', x='Variable', y='Importancia') +
  coord_flip()
)


```
```{python}
#| eval: false
vars_importance_order =list( importance_df >> arrange(_.Mean_Loss) >> pull('Variable'))

knn_reg=KNeighborsRegressor(
      n_neighbors = 16,
      weights = 'distance',
      metric ='manhattan')
      
v=[]
r2=[]
mse_=[]
for i in range(30):
  vars_to_train = vars_importance_order[i:]
  
  selectorr = ColumnTransformer([('selector', 'passthrough', vars_to_train)],
          remainder= 'drop',
          verbose_feature_names_out=False)
          
  fst_knn_pipe = Pipeline([
    ('prep',preprocessor),
    ('selector', selectorr),
    ('regressor', knn_reg)
    ])
  
  fst_knn_pipe.fit(ames_x_train, Sale_Price_train)
  pred=  fst_knn_pipe.predict( ames_x_test )
  mse_.append(get_metrics(pred,Sale_Price_test, len(vars_to_train)).loc['MSE'].values[0])
  r2.append(get_metrics(pred,Sale_Price_test, len(vars_to_train)).loc['R2Adj'].values[0])
  v.append(vars_to_train)
  
idf = pd.DataFrame({'MSE':mse_, 'R2_adj':r2,'vars':v})
best_features_from_num = (idf >> top_n(-1, _.MSE) >> pull ('vars'))[0]
```
```{python}
#| echo: false
# pd.to_pickle(idf, 'knn_num_idf.pkl')

idf = pd.read_pickle('knn_num_idf.pkl')
best_features_from_num = (idf >> top_n(-1, _.MSE) >> pull ('vars'))[0]

```


```{python}
#| eval: false
selectorr = ColumnTransformer([('selector', 'passthrough', best_features_from_num)],
          remainder= 'drop',
          verbose_feature_names_out=False).set_output(transform = 'pandas')
          
kf = KFold(n_splits=10, shuffle=True, random_state=50)

param_grid = {
 'n_neighbors': range(0, 105, 5),
 'weights': ['uniform', 'distance'],
 'metric': ['minkowski','euclidean', 'manhattan', 'hamming', 'cosine', 'chebyshev'],
 'p': range(3,10)
}
    
    
          
scnd_knn_pipe_cv = Pipeline([
  ('prep',preprocessor),
  ('selector', selectorr),
  ('cv', GridSearchCV(
      KNeighborsRegressor(), 
      param_grid, 
      cv=kf, 
      scoring=scoring, 
      refit='neg_mean_squared_error',
      verbose=3, 
      n_jobs=7))
  ])

scnd_knn_pipe_cv.fit(ames_x_train, Sale_Price_train)

```

```{python}
#| echo: false
selectorr = ColumnTransformer([('selector', 'passthrough', best_features_from_num)],
          remainder= 'drop',
          verbose_feature_names_out=False).set_output(transform = 'pandas')
# pd.to_pickle(scnd_knn_pipe_cv,'scnd_knn_pipe_cv_num.pkl')
scnd_knn_pipe_cv = pd.read_pickle('scnd_knn_pipe_cv_num.pkl')
```


```{python}

(
  pd.DataFrame(scnd_knn_pipe_cv.named_steps['cv'].cv_results_) 
  # >> filter(_.param_metric != 'chebyshev',
  #           _.param_weights == 'distance',
  #           _.param_metric != 'cosine',
            # _.param_n_neighbors >12)
  # >> mutate(min_err=_.mean_test_r2-_.std_test_r2,max_err=_.mean_test_r2+_.std_test_r2 )
  >> ggplot(aes(x = "param_n_neighbors", y = "mean_test_r2", 
             shape = "param_weights", color= 'param_metric', size='param_p')) +
  geom_point(alpha = 1,position=position_dodge(width=0.7)) +
  # geom_errorbar(aes(ymin='min_err', ymax='max_err'),
                  # position=position_dodge(width=0.7),show_legend=False) +
  ggtitle("Parametrización de KNN vs R^2") +
  # ylim([0.6,0.8])+
  xlab("Parámetro: Número de vecinos cercanos") +
  ylab('R ^2 ajustada promedio')
)
```


```{python}
#| eval: false
kf = KFold(n_splits=10, shuffle=True, random_state=53)

param_grid = {
 'n_neighbors': range(4, 30),
 'weights': ['uniform', 'distance'],
 'metric': [ 'manhattan', 'euclidean', ]
}
    
    
          
scnd_knn_pipe_cv2 = Pipeline([
  ('prep',preprocessor),
  ('selector', selectorr),
  ('cv', GridSearchCV(
      KNeighborsRegressor(), 
      param_grid, 
      cv=kf, 
      scoring=scoring, 
      refit='neg_mean_squared_error',
      verbose=3, 
      n_jobs=7))
  ])

scnd_knn_pipe_cv2.fit(ames_x_train, Sale_Price_train)

```

```{python}
#| echo: false
# pd.to_pickle(scnd_knn_pipe_cv2,'scnd_knn_pipe_cv2_num.pkl')
scnd_knn_pipe_cv2 = pd.read_pickle('scnd_knn_pipe_cv2_num.pkl')
```

```{python}
(
  pd.DataFrame(scnd_knn_pipe_cv2.named_steps['cv'].cv_results_) 
  >> filter(_.param_weights == 'distance',
            _.param_metric != 'euclidean',
            _.param_n_neighbors <10)
  >> mutate(min_err=_.mean_test_r2-_.std_test_r2,max_err=_.mean_test_r2+_.std_test_r2 )
  >> ggplot(aes(x = "param_n_neighbors", y = "mean_test_r2", 
             shape = "param_weights", color= 'param_metric')) +
  geom_point(alpha = 1,position=position_dodge(width=0.7)) +
  geom_errorbar(aes(ymin='min_err', ymax='max_err'),
                  position=position_dodge(width=0.7),show_legend=False) +
  ggtitle("Parametrización de KNN vs R^2") +
  # ylim([0.6,0.8])+
  xlab("Parámetro: Número de vecinos cercanos") +
  ylab('R ^2 ajustada promedio')
)
```

```{python}
knn_reg=KNeighborsRegressor(
      n_neighbors = 8,
      weights = 'distance',
      metric ='manhattan')
      

f_num_knn_reg = Pipeline([
  ('prep',preprocessor),
  ('selector', selectorr ),
  ('regressor', knn_reg)
  ])

f_num_knn_reg.fit(ames_x_train, Sale_Price_train)

y_pred = f_num_knn_reg.predict(ames_x_test)
get_metrics(y_pred, Sale_Price_test, len (best_features_from_num))

```
```{python}
voting = VotingRegressor(
  [('knn_num', f_num_knn_reg),
  ('knn_cat',f_cat_knn_pipeline)],
  weights =[.80,.1]
)
voting.fit(ames_x_train, Sale_Price_train)

y_pred = voting.predict(ames_x_test)
get_metrics(y_pred, Sale_Price_test, len (best_features_from_num))
```

## KNN con todas las variables

```{python}
#| eval: false
###################################################
##############  Full model ########################
###################################################

scoring = {
    'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
    'r2': make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_x_train)), p=len(num_cols)),
    'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
    'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}

 
preprocessor= ColumnTransformer([
  ('std', StandardScaler(), num_cols),
  ('onehotencoding', OneHotEncoder(drop='first',handle_unknown = 'ignore', sparse_output=False, min_frequency=15), cat_cols)
  ],
  remainder = 'drop', 
  verbose_feature_names_out= False).set_output(transform = 'pandas')



kf = KFold(n_splits=10, shuffle=True, random_state=7)


param_grid = {
 'n_neighbors': range(0, 105, 5),
 'weights': ['uniform', 'distance'],
 'metric': ['minkowski','euclidean', 'manhattan', 'hamming', 'cosine', 'chebyshev'],
 'p': range(3,10)
}
    
    
gs_knn_fm_cv = Pipeline([
  ('prep', preprocessor),
  ('cv', GridSearchCV(
      KNeighborsRegressor(), 
      param_grid, 
      cv=kf, 
      scoring=scoring, 
      refit='neg_mean_squared_error',
      verbose=3, 
      n_jobs=7))
])

gs_knn_fm_cv.fit(ames_x_train, Sale_Price_train )
```

```{python}
#| echo: false

preprocessor= ColumnTransformer([
  ('std', StandardScaler(), num_cols),
  ('onehotencoding', OneHotEncoder(drop='first',handle_unknown = 'ignore', sparse_output=False, min_frequency=15), cat_cols)
  ],
  remainder = 'drop', 
  verbose_feature_names_out= False).set_output(transform = 'pandas') 
  
# pd.to_pickle(gs_knn_fm_cv, 'gs_knn_fm_cv.pkl')
gs_knn_fm_cv = pd.read_pickle('gs_knn_fm_cv.pkl')
```


```{python}
(
  pd.DataFrame(gs_knn_fm_cv.named_steps['cv'].cv_results_) 
  # >> filter(_.param_metric != 'chebyshev',
  #           _.param_weights == 'distance',
  #           _.param_metric != 'cosine',
            # _.param_n_neighbors >12)
  # >> mutate(min_err=_.mean_test_r2-_.std_test_r2,max_err=_.mean_test_r2+_.std_test_r2 )
  >> ggplot(aes(x = "param_n_neighbors", y = "mean_test_r2", 
             shape = "param_weights", color= 'param_metric', size='param_p')) +
  geom_point(alpha = 1,position=position_dodge(width=0.7)) +
  # geom_errorbar(aes(ymin='min_err', ymax='max_err'),
                  # position=position_dodge(width=0.7),show_legend=False) +
  ggtitle("Parametrización de KNN vs R^2") +
  # ylim([0.6,0.8])+
  xlab("Parámetro: Número de vecinos cercanos") +
  ylab('R ^2 ajustada promedio')
)
```

```{python}
#| eval: false
param_grid = {
 'n_neighbors': range(3, 20),
 'weights': ['uniform', 'distance'],
 'metric': ['euclidean', 'manhattan', 'cosine']
}
    
    
gs_knn_fm_cv2 = Pipeline([
  ('prep', preprocessor),
  ('cv', GridSearchCV(
      KNeighborsRegressor(), 
      param_grid, 
      cv=kf, 
      scoring=scoring, 
      refit='neg_mean_squared_error',
      verbose=3, 
      n_jobs=7))
])

gs_knn_fm_cv2.fit(ames_x_train, Sale_Price_train )
```

```{python}
#| echo: false
# pd.to_pickle(gs_knn_fm_cv2, 'gs_knn_fm_cv2.pkl')
gs_knn_fm_cv2 = pd.read_pickle('gs_knn_fm_cv2.pkl')
```


```{python}
(
  pd.DataFrame(gs_knn_fm_cv2.named_steps['cv'].cv_results_) 
  >> filter(_.param_metric != 'chebyshev',
            _.param_weights == 'distance')
  #           _.param_metric != 'cosine',
            # _.param_n_neighbors >12)
  >> mutate(min_err=_.mean_test_r2-_.std_test_r2,max_err=_.mean_test_r2+_.std_test_r2 )
  >> ggplot(aes(x = "param_n_neighbors", y = "mean_test_r2", 
             shape = "param_weights", color= 'param_metric')) +
  geom_point(alpha = 1,position=position_dodge(width=0.7)) +
  geom_errorbar(aes(ymin='min_err', ymax='max_err'),
                  position=position_dodge(width=0.7),show_legend=False) +
  ggtitle("Parametrización de KNN vs R^2") +
  # ylim([0.6,0.8])+
  xlab("Parámetro: Número de vecinos cercanos") +
  ylab('R ^2 ajustada promedio')
)
```



```{python}

fst_knn_fm__pipe = Pipeline([
  ('prep',preprocessor),
  ('regressor', KNeighborsRegressor(
    n_neighbors = 8,
    weights = 'distance',
    metric ='manhattan')
    )]
    )
    
fst_knn_fm__pipe.fit(ames_x_train , Sale_Price_train)
y_pred  = fst_knn_fm__pipe.predict(ames_x_test)

get_metrics(y_pred, Sale_Price_test, len(preprocessor.get_feature_names_out()))
get_metrics(y_pred, Sale_Price_test, len(preprocessor.get_feature_names_out() ) )
```

```{python}
#| eval: false
knn_reg = KNeighborsRegressor(
    n_neighbors = 8,
    weights = 'distance',
    metric ='manhattan')

knn_reg.fit(preprocessor.fit_transform(ames_x_train), Sale_Price_train)


importance_df = importance_from_model(
  test_frame = transformed_test_df,
  y_obs = Sale_Price_test ,
  selected_columns = transformed_test_df.columns.to_list(), 
  pipeline = knn_reg,
  actual_mse =get_metrics(y_pred, Sale_Price_test, len(preprocessor.get_feature_names_out())).loc['MSE'].values[0] ,
  n_permutations= 50,
  trans_pred= False
)
```
```{python}
#| echo: false
#| 
transformed_test_df = preprocessor.transform(ames_x_test)
# pd.to_pickle(importance_df , 'importance_df_fm.pkl')
importance_df = pd.read_pickle('importance_df_fm.pkl')

```

```{python}
(
  importance_df >> top_n(30, _.Mean_Loss)>>
  mutate(
    ymin = _.Mean_Loss - _.Std_Loss,
    ymax = _.Mean_Loss + _.Std_Loss) >>
  ggplot(aes(x = 'reorder(Variable, Mean_Loss)', y = "Mean_Loss")) +
  geom_errorbar(aes(ymin='ymin', ymax='ymax'),
    width=0.2, position=position_dodge(0.9)) +
  geom_point(alpha = 0.65) +
  labs(title='Importancia de las Variables', x='Variable', y='Importancia') +
  coord_flip()
)
(
  importance_df >> top_n(-30, _.Mean_Loss)>>
  mutate(
    ymin = _.Mean_Loss - _.Std_Loss,
    ymax = _.Mean_Loss + _.Std_Loss) >>
  ggplot(aes(x = 'reorder(Variable, Mean_Loss)', y = "Mean_Loss")) +
  geom_errorbar(aes(ymin='ymin', ymax='ymax'),
    width=0.2, position=position_dodge(0.9)) +
  geom_point(alpha = 0.65) +
  labs(title='Importancia de las Variables', x='Variable', y='Importancia') +
  coord_flip()
)

```

```{python}
#| eval: false
vars_importance_order = list( importance_df >> arrange(_.Mean_Loss) >> pull('Variable'))
transformed_df = preprocessor.fit_transform(ames_x_train)
v=[]
r2=[]
mse_=[]
for i in range(150):
  vars_to_train = vars_importance_order[i:]
  
  knn_reg.fit(transformed_df[vars_to_train], Sale_Price_train)
  pred= knn_reg.predict(transformed_test_df[vars_to_train])
  mse_.append(get_metrics(pred,Sale_Price_test, len(vars_to_train)).loc['MSE'].values[0])
  r2.append(get_metrics(pred,Sale_Price_test, len(vars_to_train)).loc['R2Adj'].values[0])
  v.append(vars_to_train)
idf = pd.DataFrame({'MSE':mse_, 'R2_adj':r2,'vars':v})
best_features_from_fm = (idf >> mutate(MSE= (_.MSE)**(0.5) ) >> filter(_.MSE < 33500) >> top_n(1, _.R2_adj) >> pull('vars'))[0]
```


```{python}
#| echo: false

# pd.to_pickle(idf, 'idf_fm_knn.pkl')

idf= pd.read_pickle('idf_fm_knn.pkl')
best_features_from_fm = (idf >> mutate(MSE= (_.MSE)**(0.5) ) >> filter(_.MSE < 33500) >> top_n(1, _.R2_adj) >> pull('vars'))[0]
```


```{python}
#| eval: false

preprocessor= ColumnTransformer([
  ('std', StandardScaler(), num_cols),
  ('onehotencoding', OneHotEncoder(drop='first',handle_unknown = 'ignore', sparse_output=False, min_frequency=15), cat_cols)
  ],
  remainder = 'drop', 
  verbose_feature_names_out= False).set_output(transform = 'pandas')

selectorr = ColumnTransformer(
  [('selector', 'passthrough',best_features_from_fm)],
  remainder = 'drop',
  verbose_feature_names_out= False).set_output(transform = 'pandas')



kf = KFold(n_splits=10, shuffle=True, random_state=77)


param_grid = {
 'n_neighbors': range(0, 50, 1),
 'weights': ['uniform', 'distance'],
 'metric': ['minkowski','euclidean', 'manhattan', 'hamming', 'cosine', 'chebyshev'],
 'p': range(3,10)
}
    
    
final_cv_knn = Pipeline([
  ('prep', preprocessor),
  ('selector', selectorr),
  ('cv', GridSearchCV(
      KNeighborsRegressor(), 
      param_grid, 
      cv=kf, 
      scoring=scoring, 
      refit='neg_mean_squared_error',
      verbose=3, 
      n_jobs=7))
])

final_cv_knn.fit(ames_x_train, Sale_Price_train )

```


```{python}
#| echo: false

# pd.to_pickle(final_cv_knn, 'final_cv_knn_fm.pkl')
final_cv_knn = pd.read_pickle('final_cv_knn_fm.pkl')
```


```{python}
(
  pd.DataFrame(final_cv_knn.named_steps['cv'].cv_results_) 
  >> filter(_.param_metric != 'chebyshev',
            _.param_weights == 'distance',
            _.param_metric == 'manhattan',
            _.param_metric != 'hamming',
            _.param_n_neighbors <22)
  >> mutate(min_err=_.mean_test_r2-_.std_test_r2,max_err=_.mean_test_r2+_.std_test_r2 )
  >> ggplot(aes(x = "param_n_neighbors", y = "mean_test_r2", 
             shape = "param_weights", color= 'param_metric')) +
  geom_point(alpha = 1,position=position_dodge(width=0.7)) +
  geom_errorbar(aes(ymin='min_err', ymax='max_err'),
                  position=position_dodge(width=0.7),show_legend=False) +
  ggtitle("Parametrización de KNN vs R^2") +
  # ylim([0.6,0.8])+
  xlab("Parámetro: Número de vecinos cercanos") +
  ylab('R ^2 ajustada promedio')
)
```

```{python}
      

f_num_knn_reg_fm = Pipeline([
  ('prep',preprocessor),
  ('selector', selectorr ),
  ('regressor', final_cv_knn.named_steps['cv'].best_estimator_)
  ])

f_num_knn_reg_fm .fit(ames_x_train, Sale_Price_train)

y_pred = f_num_knn_reg_fm .predict(ames_x_test)
get_metrics(y_pred, Sale_Price_test, len (best_features_from_fm))
```
```{python}
voting = VotingRegressor(
  [('knn_num', f_num_knn_reg),
  ('knn_fm', f_num_knn_reg_fm)],
  weights  = [0.9, 0.1]
)
m1 = list(f_num_knn_reg.named_steps['selector'].get_feature_names_out())
m3 = list(f_num_knn_reg_fm.named_steps['selector'].get_feature_names_out())
predictors =list( set(m1+m3 ))



voting.fit(ames_x_train, Sale_Price_train)

y_pred = voting.predict(ames_x_test)
get_metrics(y_pred, Sale_Price_test, len(predictors) )
```

```{python}
y_obs = Sale_Price_test
ames_test = (
  ames_x_test >>
  mutate(Sale_Price_Pred = y_pred, Sale_Price = Sale_Price_test)
)
(
    ggplot(aes(x = y_obs, y = y_pred)) +
    geom_point(alpha=0.5) +
    scale_y_continuous(labels = dollar_format(digits=0, big_mark=','), limits = [0, 600000] ) +
    scale_x_continuous(labels = dollar_format(digits=0, big_mark=','), limits = [0, 600000]) +
    geom_abline(color = "red") +
    coord_equal() +
    labs(
      title = "Comparación entre predicción y observación",
      x = "Predicción",
      y = "Observación")
)


(
ames_test >>
  select(_.Sale_Price, _.Sale_Price_Pred) >>
  mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
  ggplot(aes(x = "error")) +
  geom_histogram(color = "white", fill = "black") +
  geom_vline(xintercept = 0, color = "red") +
  scale_x_continuous(labels=dollar_format(big_mark=',', digits=0)) + 
  ylab("Conteos de clase") + xlab("Errores") +
  ggtitle("Distribución de error")
)


(
ames_test >>
  select(_.Sale_Price, _.Sale_Price_Pred) >>
  mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
  ggplot(aes(sample = "error")) +
  geom_qq(alpha = 0.3) + stat_qq_line(color = "red") +
  scale_y_continuous(labels=dollar_format(big_mark=',', digits = 0)) + 
  xlab("Distribución normal") + ylab("Distribución de errores") +
  ggtitle("QQ-Plot")
)


(
ames_test >>
  select(_.Sale_Price, _.Sale_Price_Pred) >>
  mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
  ggplot(aes(x = "Sale_Price")) +
  geom_linerange(aes(ymin = 0, ymax = "error"), colour = "purple") +
  geom_point(aes(y = "error"), size = 0.05, alpha = 0.5) +
  geom_abline(intercept = 0, slope = 0) +
  scale_x_continuous(labels=dollar_format(big_mark=',', digits=0)) + 
  scale_y_continuous(labels=dollar_format(big_mark=',', digits=0)) +
  xlab("Precio real") + ylab("Error de estimación") +
  ggtitle("Relación entre error y precio de venta")
)
```

```{python}
# validation_results(
#       x_val = ames_x_val,
#       y_val = Sale_Price_validation, 
#       pipeline = voting, 
#       n_preds=18)
      
```

