})
return importance_df
def importance_from_model (test_frame, selected_columns, pipeline, n_permutations=50 , actual_mse):
performance_losses = []
for i in range(test_frame[selected_columns].shape[1]):
loss = []
for j in range(n_permutations):
test_frame_permuted = test_frame[selected_columns].copy()
test_frame_permuted.iloc[:, i] = np.random.permutation(test_frame_permuted.iloc[:, i])
y_pred_permuted = pipe.predict(test_frame_permuted)
mse_permuted = mean_squared_error(y_obs, y_pred_permuted)
loss.append(mse_permuted)
performance_losses.append(loss)
performance_losses = performance_losses/np.sum(performance_losses, axis=0)
mean_losses = np.mean(performance_losses, axis=1)
std_losses = np.std(performance_losses, axis=1)
importance_df = pd.DataFrame({
'Variable': selected_columns,
'Mean_Loss': mean_losses,
'Std_Loss': std_losses
})
return importance_df
def importance_from_model (test_frame, selected_columns, pipeline, n_permutations=50 , actual_mse):
performance_losses = []
def importance_from_model (test_frame, selected_columns, pipeline, actual_mse, n_permutations=50 ):
performance_losses = []
for i in range(test_frame[selected_columns].shape[1]):
loss = []
for j in range(n_permutations):
test_frame_permuted = test_frame[selected_columns].copy()
test_frame_permuted.iloc[:, i] = np.random.permutation(test_frame_permuted.iloc[:, i])
y_pred_permuted = pipe.predict(test_frame_permuted)
mse_permuted = mean_squared_error(y_obs, y_pred_permuted)
loss.append(mse_permuted)
performance_losses.append(loss)
performance_losses = performance_losses/np.sum(performance_losses, axis=0)
mean_losses = np.mean(performance_losses, axis=1)
std_losses = np.std(performance_losses, axis=1)
importance_df = pd.DataFrame({
'Variable': selected_columns,
'Mean_Loss': mean_losses,
'Std_Loss': std_losses
})
return importance_df
selected_important = important_num_cols + important_cat_cols
pd.DataFrame(get_metrics(y_pred, y_obs, p))
mse = mean_squared_error(y_obs, y_pred)
mse
importance_from_model(ames_x_test,selected_important, pipe,actual_mse=mse, n_permutations=10 )
y_obs
def importance_from_model (test_frame, y_obs, selected_columns, pipeline, actual_mse, n_permutations=50 ):
performance_losses = []
for i in range(test_frame[selected_columns].shape[1]):
loss = []
for j in range(n_permutations):
test_frame_permuted = test_frame[selected_columns].copy()
test_frame_permuted.iloc[:, i] = np.random.permutation(test_frame_permuted.iloc[:, i])
y_pred_permuted = pipeline.predict(test_frame_permuted)* test_frame_permuted.Gr_Liv_Area
mse_permuted = mean_squared_error(y_obs, y_pred_permuted)
loss.append(mse_permuted)
performance_losses.append(loss)
performance_losses = performance_losses/np.sum(performance_losses, axis=0)
mean_losses = np.mean(performance_losses, axis=1)
std_losses = np.std(performance_losses, axis=1)
importance_df = pd.DataFrame({
'Variable': selected_columns,
'Mean_Loss': mean_losses,
'Std_Loss': std_losses
})
return importance_df
df= importance_from_model(ames_x_test,y_obs, selected_important, pipe,actual_mse=mse, n_permutations= 50 )
df
(
df >>
mutate(
ymin = _.Mean_Loss - _.Std_Loss,
ymax = _.Mean_Loss + _.Std_Loss) >>
arrange(_.Mean_Loss)>> top_n(35) >>
ggplot(aes(x = 'reorder(Variable, Mean_Loss)', y = "Mean_Loss")) +
geom_errorbar(aes(ymin='ymin', ymax='ymax'),
width=0.2, position=position_dodge(0.9)) +
geom_point(alpha = 0.65) +
labs(title='Importancia de las Variables', x='Variable', y='Importancia') +
coord_flip()
)
(
df >> filter(_.Variable!= 'Gr_Liv_Area')>>
mutate(
ymin = _.Mean_Loss - _.Std_Loss,
ymax = _.Mean_Loss + _.Std_Loss) >>
arrange(_.Mean_Loss)>> top_n(35) >>
ggplot(aes(x = 'reorder(Variable, Mean_Loss)', y = "Mean_Loss")) +
geom_errorbar(aes(ymin='ymin', ymax='ymax'),
width=0.2, position=position_dodge(0.9)) +
geom_point(alpha = 0.65) +
labs(title='Importancia de las Variables', x='Variable', y='Importancia') +
coord_flip()
)
(
df >> filter(_.Variable!= 'Gr_Liv_Area')>>
mutate(
ymin = _.Mean_Loss - _.Std_Loss,
ymax = _.Mean_Loss + _.Std_Loss) >>
arrange(_.Mean_Loss)>> top_n(35) >>
ggplot(aes(x = 'reorder(Variable, Mean_Loss)', y = "Mean_Loss")) +
geom_errorbar(aes(ymin='ymin', ymax='ymax'),
width=0.2, position=position_dodge(0.9)) +
geom_point(alpha = 0.65) +
labs(title='Importancia de las Variables', x='Variable', y='Importancia') +
coord_flip()
)
(
df >> filter(_.Variable!= 'Gr_Liv_Area')>>
mutate(
ymin = _.Mean_Loss - _.Std_Loss,
ymax = _.Mean_Loss + _.Std_Loss) >>
arrange(_.Mean_Loss)>> top_n(35) >>
ggplot(aes(x = 'reorder(Variable, Mean_Loss)', y = "Mean_Loss")) +
geom_errorbar(aes(ymin='ymin', ymax='ymax'),
width=0.2, position=position_dodge(0.9)) +
geom_point(alpha = 0.65) +
labs(title='Importancia de las Variables', x='Variable', y='Importancia') +
coord_flip()
)
(
df >> filter(_.Variable!= 'Gr_Liv_Area')>>
mutate(
ymin = _.Mean_Loss - _.Std_Loss,
ymax = _.Mean_Loss + _.Std_Loss) >>
arrange(_.Mean_Loss)>> top_n(35) >>
ggplot(aes(x = 'reorder(Variable, Mean_Loss)', y = "Mean_Loss")) +
geom_errorbar(aes(ymin='ymin', ymax='ymax'),
width=0.2, position=position_dodge(0.9)) +
geom_point(alpha = 0.65) +
labs(title='Importancia de las Variables', x='Variable', y='Importancia') +
coord_flip()
)
def importance_from_model (test_frame, y_obs, selected_columns, pipeline, actual_mse, n_permutations=50 ,trans_pred= False ):
performance_losses = []
for i in range(test_frame[selected_columns].shape[1]):
loss = []
for j in range(n_permutations):
test_frame_permuted = test_frame[selected_columns].copy()
test_frame_permuted.iloc[:, i] = np.random.permutation(test_frame_permuted.iloc[:, i])
if trans_pred == False:
y_pred_permuted = pipeline.predict(test_frame_permuted)
else:
y_pred_permuted = pipeline.predict(test_frame_permuted)* test_frame_permuted.Gr_Liv_Area
mse_permuted = mean_squared_error(y_obs, y_pred_permuted)
loss.append(mse_permuted)
performance_losses.append(loss)
performance_losses = performance_losses/np.sum(performance_losses, axis=0)
mean_losses = np.mean(performance_losses, axis=1)
std_losses = np.std(performance_losses, axis=1)
importance_df = pd.DataFrame({
'Variable': selected_columns,
'Mean_Loss': mean_losses,
'Std_Loss': std_losses
})
return importance_df
def get_metrics (y_pred, y_obs,predictors):
me = np.mean(y_obs - y_pred)
mae = mean_absolute_error(y_obs, y_pred)
mape = mean_absolute_percentage_error(y_obs, y_pred)
mse = mean_squared_error(y_obs, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_obs, y_pred)
n = len(y_obs)  # Número de observaciones
p = predictors  # Número de predictores
r2_adj = 1 - (n - 1) / (n - p - 1) * (1 - r2)
metrics_data = {
"Metric": ["ME", "MAE", "MAPE", "MSE", "RMSE", "R^2", "R^2 Adj"],
"Value": [me, mae, mape, mse, rmse, r2, r2_adj]
}
return pd.DataFrame(metrics_data).set_index('Metric')
get_metrics(y_pred, y_obs, p)
def get_metrics (y_pred, y_obs,predictors):
me = np.mean(y_obs - y_pred)
mae = mean_absolute_error(y_obs, y_pred)
mape = mean_absolute_percentage_error(y_obs, y_pred)
mse = mean_squared_error(y_obs, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_obs, y_pred)
n = len(y_obs)  # Número de observaciones
p = predictors  # Número de predictores
r2_adj = 1 - (n - 1) / (n - p - 1) * (1 - r2)
metrics_data = {
"Metric": ["ME", "MAE", "MAPE", "MSE", "RMSE", "R2", "R2Adj"],
"Value": [me, mae, mape, mse, rmse, r2, r2_adj]
}
return pd.DataFrame(metrics_data).set_index('Metric')
get_metrics(y_pred, y_obs, p)
s=get_metrics(y_pred, y_obs, p)
s.R2Adj
s['R2Adj']
s
s.iloc[-1]
s.iloc['R2Adj']
s.loc['R2Adj']
reticulate::repl_python()
#| label: intial_split
#| echo: true
from sklearn.model_selection import train_test_split
from plydata.one_table_verbs import pull
from mizani.formatters import comma_format, dollar_format
ames = pd.read_csv("../data/ames.csv") >> mutate (Price_4_GLA= _.Sale_Price/_.Gr_Liv_Area)
y = ames >>  pull("Price_4_GLA")
X = select(ames, -_.Sale_Price, -_.Price_4_GLA)
X=(X >> mutate(wood_prop= _.Wood_Deck_SF/_.Gr_Liv_Area,
area_per_car=case_when({
_.Garage_Cars == 0:0,
True:_.Garage_Area/ _.Garage_Cars}),
last_remod = case_when({
_.Year_Built>_.Year_Remod_Add: 2023-_.Year_Remod_Add,
True: 2023-_.Year_Built}),
Mo_Sold= _.Mo_Sold.astype('object')))
numeric_column = ames >> pull("Price_4_GLA")
quartiles = np.percentile(numeric_column, [25, 50, 75])
# Crea una nueva variable categórica basada en los cuartiles
stratify_variable = pd.cut(
numeric_column,
bins=[float('-inf'), quartiles[0], quartiles[1], quartiles[2], float('inf')],
labels=["Q1", "Q2", "Q3", "Q4"]
)
ames_x_train, ames_x, ames_y_train, ames_y = train_test_split(
X, y,
test_size = 0.30,
random_state = 12345,
stratify = stratify_variable
)
stratify_variable = pd.cut(
ames_y ,
bins=[float('-inf'), quartiles[0], quartiles[1], quartiles[2], float('inf')],
labels=["Q1", "Q2", "Q3", "Q4"]
)
ames_x_test, ames_x_val, ames_y_test, ames_y_val = train_test_split(
ames_x, ames_y,
test_size = 1/3,
random_state = 12345,
stratify = stratify_variable
)
print('Totales:\t', ames.shape[0],'\nTraining:\t', ames_y_train.shape[0],'\nTesting:\t', ames_y_test.shape[0],'\nValidation:\t', ames_y_val.shape[0])
#| label: load-py-packages
#| include: false
import pandas as pd
from siuba import *
import numpy as np
from plotnine import *
import plydata as pr
from plydata.tidy import pivot_wider, pivot_longer
import matplotlib.pyplot as plt
#| label: intial_split
#| echo: true
from sklearn.model_selection import train_test_split
from plydata.one_table_verbs import pull
from mizani.formatters import comma_format, dollar_format
ames = pd.read_csv("../data/ames.csv") >> mutate (Price_4_GLA= _.Sale_Price/_.Gr_Liv_Area)
y = ames >>  pull("Price_4_GLA")
X = select(ames, -_.Sale_Price, -_.Price_4_GLA)
X=(X >> mutate(wood_prop= _.Wood_Deck_SF/_.Gr_Liv_Area,
area_per_car=case_when({
_.Garage_Cars == 0:0,
True:_.Garage_Area/ _.Garage_Cars}),
last_remod = case_when({
_.Year_Built>_.Year_Remod_Add: 2023-_.Year_Remod_Add,
True: 2023-_.Year_Built}),
Mo_Sold= _.Mo_Sold.astype('object')))
numeric_column = ames >> pull("Price_4_GLA")
quartiles = np.percentile(numeric_column, [25, 50, 75])
# Crea una nueva variable categórica basada en los cuartiles
stratify_variable = pd.cut(
numeric_column,
bins=[float('-inf'), quartiles[0], quartiles[1], quartiles[2], float('inf')],
labels=["Q1", "Q2", "Q3", "Q4"]
)
ames_x_train, ames_x, ames_y_train, ames_y = train_test_split(
X, y,
test_size = 0.30,
random_state = 12345,
stratify = stratify_variable
)
stratify_variable = pd.cut(
ames_y ,
bins=[float('-inf'), quartiles[0], quartiles[1], quartiles[2], float('inf')],
labels=["Q1", "Q2", "Q3", "Q4"]
)
ames_x_test, ames_x_val, ames_y_test, ames_y_val = train_test_split(
ames_x, ames_y,
test_size = 1/3,
random_state = 12345,
stratify = stratify_variable
)
print('Totales:\t', ames.shape[0],'\nTraining:\t', ames_y_train.shape[0],'\nTesting:\t', ames_y_test.shape[0],'\nValidation:\t', ames_y_val.shape[0])
importance_df
### Fuciones personalizadas
quit
reticulate::source_python('~/CD_amat_py/Custom_functions_ ames.py')
reticulate::source_python('~/CD_amat_py/Custom_functions_ ames.py')
reticulate::repl_python()
### Fuciones personalizadas
###### IMPORTTANCIA DE VARIABLES
#     PARAMETROS:
##  test_fram:          dataframe de las features para testing
##  y_obs:              labes de los datos de testing
##  seleccted_columns:  Variables que contempla el modelo\
##  actual_mse:         MSE calculado en test con todas las variables
##  pipeline:           pipeline para hacer las predcciones
##  trans_pred:         Poner True si las predicciones se están haciendo por
## n_permutations=50
def importance_from_model (test_frame, y_obs, selected_columns, pipeline, actual_mse, n_permutations=50 ,trans_pred= False ):
performance_losses = []
for i in range(test_frame[selected_columns].shape[1]):
loss = []
for j in range(n_permutations):
test_frame_permuted = test_frame[selected_columns].copy()
test_frame_permuted.iloc[:, i] = np.random.permutation(test_frame_permuted.iloc[:, i])
if trans_pred == False:
y_pred_permuted = pipeline.predict(test_frame_permuted)
else:
y_pred_permuted = pipeline.predict(test_frame_permuted)* test_frame_permuted.Gr_Liv_Area
mse_permuted = mean_squared_error(y_obs, y_pred_permuted)
loss.append(mse_permuted)
performance_losses.append(loss)
performance_losses = performance_losses/np.sum(performance_losses, axis=0)
mean_losses = np.mean(performance_losses, axis=1)
std_losses = np.std(performance_losses, axis=1)
importance_df = pd.DataFrame({
'Variable': selected_columns,
'Mean_Loss': mean_losses,
'Std_Loss': std_losses
})
return importance_df
def div_columns(X, c1, c2):
X["c1_c2"] = X[c1]/ X[c2]
return X
def adj_r(y_o, y_p , p, n):
r2_adj = 1 - (n - 1) / (n - p - 1) * (1 - r2_score(y_o, y_p))
def get_metrics (y_pred, y_obs,predictors):
me = np.mean(y_obs - y_pred)
mae = mean_absolute_error(y_obs, y_pred)
mape = mean_absolute_percentage_error(y_obs, y_pred)
mse = mean_squared_error(y_obs, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_obs, y_pred)
n = len(y_obs)  # Número de observaciones
p = predictors  # Número de predictores
r2_adj = 1 - (n - 1) / (n - p - 1) * (1 - r2)
metrics_data = {
"Metric": ["ME", "MAE", "MAPE", "MSE", "RMSE", "R2", "R2Adj"],
"Value": [me, mae, mape, mse, rmse, r2, r2_adj]
}
return pd.DataFrame(metrics_data).set_index('Metric')
def rmv_elements (list_of_elements, complete_list):
for element in list_of_elements:
complete_list.remove(element)
return complete_list
def validation_results(x_val,y_val,  pipeline , n_preds):
y_pred = pipeline.predict(x_val)
ames_test = (
ames_x_val >>
mutate(Sale_Price_Pred = y_pred*_.Gr_Liv_Area, Sale_Price =y_val*_.Gr_Liv_Area))
##### Métricas de desempeño
y_obs = ames_test["Sale_Price"]
y_pred = ames_test["Sale_Price_Pred"]
return get_metrics(y_pred, y_obs, n_preds)
preprocessor= ColumnTransformer(
transformers = [
('scaler', StandardScaler(), num_cols),
],
verbose_feature_names_out = False,
remainder = 'drop'  # Mantener las columnas restantes sin cambios
).set_output(transform="pandas")
#####   C R O S S   V A L I D A T I O N   #####
##### Just numeric KNN
k = 10
kf = KFold(n_splits=k, shuffle=True, random_state=42)
param_grid = {
'n_neighbors': range(5, 26),
'weights': ['distance'],
'metric': ['minkowski'],
'p': range(3,10)
}
from mlxtend.feature_selection import ColumnSelector
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate
from sklearn.model_selection import GridSearchCV
from sklearn.utils import shuffle
from plydata.one_table_verbs import pull
from mizani.formatters import comma_format, dollar_format
from plotnine import *
from siuba import *
import pandas as pd
import numpy as np
from sklearn.ensemble import VotingRegressor
cat_columns = ['MS_SubClass',
'MS_Zoning',
'Street',
'Alley',
'Lot_Shape',
'Land_Contour',
'Utilities',
'Lot_Config',
'Land_Slope',
'Neighborhood',
'Condition_1',
'Condition_2',
'Bldg_Type',
'House_Style',
'Overall_Cond',
'Roof_Style',
'Roof_Matl',
'Exterior_1st',
'Exterior_2nd',
'Mas_Vnr_Type',
'Exter_Cond',
'Foundation',
'Bsmt_Cond',
'Bsmt_Exposure',
'BsmtFin_Type_1',
'BsmtFin_Type_2',
'Heating',
'Heating_QC',
'Central_Air',
'Electrical',
'Functional',
'Garage_Type',
'Garage_Finish',
'Garage_Cond',
'Paved_Drive',
'Pool_QC',
'Fence',
'Misc_Feature',
'Mo_Sold',
# 'Sale_Type',
# 'Sale_Condition'
]
num_cols =[ 'Bedroom_AbvGr',
'BsmtFin_SF_1',
'BsmtFin_SF_2',
'Bsmt_Full_Bath',
'Bsmt_Half_Bath',
'Bsmt_Unf_SF',
'Enclosed_Porch',
'Fireplaces',
'First_Flr_SF',
'Full_Bath',
'Garage_Area',
'Garage_Cars',
'Gr_Liv_Area',
'Half_Bath',
'Kitchen_AbvGr',
'Latitude',
'Longitude',
'Lot_Area',
'Lot_Frontage',
'Mas_Vnr_Area',
'Misc_Val',
'Open_Porch_SF',
'Pool_Area',
'Screen_Porch',
'Second_Flr_SF',
'Three_season_porch',
'TotRms_AbvGrd',
'Total_Bsmt_SF',
'Wood_Deck_SF',
'Year_Built',
'Year_Remod_Add',
'Year_Sold',
'area_per_car',
'last_remod',
'wood_prop']
preprocessor= ColumnTransformer(
transformers = [
('scaler', StandardScaler(), num_cols),
],
verbose_feature_names_out = False,
remainder = 'drop'  # Mantener las columnas restantes sin cambios
).set_output(transform="pandas")
#####   C R O S S   V A L I D A T I O N   #####
##### Just numeric KNN
k = 10
kf = KFold(n_splits=k, shuffle=True, random_state=42)
param_grid = {
'n_neighbors': range(5, 26),
'weights': ['distance'],
'metric': ['minkowski'],
'p': range(3,10)
}
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(adjusted_r2_score, greater_is_better=True, n=np.ceil(len(ames_x_train)), p=len(num_cols)),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
pipeline_knn_numeric_cv = Pipeline(
[('prep', preprocessor),
('column_selection', column_selector),
('regressor', GridSearchCV(
KNeighborsRegressor(),
param_grid,
cv=kf,
scoring=scoring,
refit='neg_mean_squared_error',
verbose=3,
n_jobs=7)
)]
)
