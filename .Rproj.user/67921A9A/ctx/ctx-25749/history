)+
theme(figure_size=(8, 6))
)
# plt.show()
#| results: hide
#| fig-keep: 'all'
from mizani.formatters import comma_format, dollar_format
ames_train = (X_train >> mutate(Sale_Price= y_train))
plt.figure()
(
ames_train >> ggplot(aes(y='Sale_Price', x=0))+
geom_violin( fill='#702C27',
style='right',
alpha=0.6)+
geom_jitter(aes(x=-0.21),
size=0.8,
alpha=0.6,
color='#8E8058',
width=0.2,
random_state=7)+
geom_boxplot(aes(x=-0.21),width=0.4,
color='#182E40',
alpha=0.1,
outlier_alpha=1,
outlier_color='#235C6C',
outlier_size=1.5)+
coord_flip()+
theme_classic()+
ggtitle('Distribución de los precios de venta')+
scale_y_continuous(labels=dollar_format(big_mark=','))+
ylab('Precio de venta')+
theme(axis_ticks_minor_y=element_blank(),
axis_ticks_major_y=element_blank(),
axis_text_y=element_blank()
)+
theme(figure_size=(8, 6))
)
# plt.show()
library(reticulate)
use_virtualenv("CD_AMAT_2023")
reticulate::repl_python()
from mlxtend.feature_selection import ColumnSelector
from mlxtend.feature_selection import ColumnSelector
import mlxtend as mlx
exit
virtualenv_install(envname = "CD_AMAT_2023", packages = 'mlxtend')
reticulate::repl_python()
from mlxtend.feature_selection import ColumnSelector
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate
from plydata.one_table_verbs import pull
from mizani.formatters import comma_format, dollar_format
ames = pd.read_csv("data/ames.csv")
ames_y = ames >> pull("Sale_Price")    # ames[["Sale_Price"]]
ames_x = select(ames, -_.Sale_Price)   # ames.drop('Sale_Price', axis=1)
#### DIVISIÓN DE DATOS ####
ames_x_train, ames_x_test, ames_y_train, ames_y_test = train_test_split(
ames_x, ames_y,
test_size = 0.20,
random_state = 195
)
#### FEATURE ENGINEERING ####
## SELECCIÓN DE VARIABLES
# Seleccionamos las variales numéricas de interés
num_cols = ["Full_Bath", "Half_Bath"]
# Seleccionamos las variables categóricas de interés
cat_cols = ["Overall_Cond"]
# Juntamos todas las variables de interés
columnas_seleccionadas = num_cols + cat_cols
pipe = ColumnSelector(columnas_seleccionadas)
ames_x_train_selected = pipe.fit_transform(ames_x_train)
ames_train_selected = pd.DataFrame(
ames_x_train_selected,
columns = columnas_seleccionadas
)
ames_train_selected.info()
## TRANSFORMACIÓN DE COLUMNAS
# ColumnTransformer para aplicar transformaciones
preprocessor = ColumnTransformer(
transformers = [
('scaler', StandardScaler(), num_cols),
('onehotencoding', OneHotEncoder(drop='first'), cat_cols)
],
verbose_feature_names_out = False,
remainder = 'passthrough'  # Mantener las columnas restantes sin cambios
)
transformed_data = preprocessor.fit_transform(ames_train_selected)
new_column_names = preprocessor.get_feature_names_out()
transformed_df = pd.DataFrame(
transformed_data.todense(),
columns=new_column_names
)
transformed_df
transformed_df.info()
#### PIPELINE Y MODELADO
# Crear el pipeline con la regresión lineal
pipeline = Pipeline([
('preprocessor', preprocessor),
('regressor', LinearRegression())
])
# Entrenar el pipeline
results = pipeline.fit(ames_train_selected, ames_y_train)
## PREDICCIONES
y_pred = pipeline.predict(ames_x_test)
ames_test = (
ames_x_test >>
mutate(Sale_Price_Pred = y_pred, Sale_Price = ames_y_test)
)
ames_test.info()
(
ames_test >>
select(_.Sale_Price, _.Sale_Price_Pred)
)
##### Extracción de coeficientes
X_train_with_intercept = sm.add_constant(transformed_df)
model = sm.OLS(ames_y_train, X_train_with_intercept).fit()
model.summary()
##### Métricas de desempeño
pd.options.display.float_format = '{:.2f}'.format
y_obs = ames_test["Sale_Price"]
y_pred = ames_test["Sale_Price_Pred"]
me = np.mean(y_obs - y_pred)
mae = mean_absolute_error(y_obs, y_pred)
mape = mean_absolute_percentage_error(y_obs, y_pred)
mse = mean_squared_error(y_obs, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_obs, y_pred)
n = len(y_obs)  # Número de observaciones
p = 9  # Número de predictores
r2_adj = 1 - (n - 1) / (n - p - 1) * (1 - r2)
metrics_data = {
"Metric": ["ME", "MAE", "MAPE", "MSE", "RMSE", "R^2", "R^2 Adj"],
"Value": [me, mae, mape, mse, rmse, r2, r2_adj]
}
metrics_df = pd.DataFrame(metrics_data)
metrics_df
#### Gráficos de desempeño de modelo
(
ames_test >>
ggplot(aes(x = "Sale_Price_Pred", y = "Sale_Price")) +
geom_point() +
scale_y_continuous(labels = dollar_format(digits=0, big_mark=','), limits = [0, 600000] ) +
scale_x_continuous(labels = dollar_format(digits=0, big_mark=','), limits = [0, 500000] ) +
geom_abline(color = "red") +
coord_equal() +
labs(
title = "Comparación entre predicción y observación",
x = "Predicción",
y = "Observación")
)
(
ames_test >>
select(_.Sale_Price, _.Sale_Price_Pred) >>
mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
ggplot(aes(x = "error")) +
geom_histogram(color = "white", fill = "black") +
geom_vline(xintercept = 0, color = "red") +
scale_x_continuous(labels=dollar_format(big_mark=',', digits=0)) +
ylab("Conteos de clase") + xlab("Errores") +
ggtitle("Distribución de error")
)
(
ames_test >>
select(_.Sale_Price, _.Sale_Price_Pred) >>
mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
ggplot(aes(sample = "error")) +
geom_qq(alpha = 0.3) + stat_qq_line(color = "red") +
scale_y_continuous(labels=dollar_format(big_mark=',', digits = 0)) +
xlab("Distribución normal") + ylab("Distribución de errores") +
ggtitle("QQ-Plot")
)
(
ames_test >>
select(_.Sale_Price, _.Sale_Price_Pred) >>
mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
ggplot(aes(x = "Sale_Price")) +
geom_linerange(aes(ymin = 0, ymax = "error"), colour = "purple") +
geom_point(aes(y = "error"), size = 0.05, alpha = 0.5) +
geom_abline(intercept = 0, slope = 0) +
scale_x_continuous(labels=dollar_format(big_mark=',', digits=0)) +
scale_y_continuous(labels=dollar_format(big_mark=',', digits=0)) +
xlab("Precio real") + ylab("Error de estimación") +
ggtitle("Relación entre error y precio de venta")
)
#### Validación cruzada ####
# Definir el objeto K-Fold Cross Validator
kf = KFold(n_splits=10, shuffle=True, random_state=42)
# Definir las métricas de desempeño que deseas calcular como funciones de puntuación
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(r2_score),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
# Realizar la validación cruzada y calcular métricas de desempeño utilizando cross_val_score
results = cross_validate(
pipeline,
ames_train_selected, ames_y_train,
cv=kf,
scoring=scoring
)
# Calcular estadísticas resumidas (media y desviación estándar) de las métricas
mean_rmse = np.mean(np.sqrt(-results['test_neg_mean_squared_error']))
std_rmse = np.std(np.sqrt(-results['test_neg_mean_squared_error']))
mean_r2 = np.mean(results['test_r2'])
std_r2 = np.std(results['test_r2'])
mean_mae = np.mean(-results['test_neg_mean_absolute_error'])
std_mae = np.std(-results['test_neg_mean_absolute_error'])
mean_mape = np.mean(-results['test_mape'])
std_mape = np.std(-results['test_mape'])
λ
# Imprimir los resultados
print(f"MAE: {mean_mae} +/- {std_mae}")
print(f"MAPE: {mean_mape} +/- {std_mape}")
print(f"R^2: {mean_r2} +/- {std_r2}")
print(f"RMSE: {mean_rmse} +/- {std_rmse}")
# pip install mlxtend==0.23.0
from mlxtend.feature_selection import ColumnSelector
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate
from plydata.one_table_verbs import pull
from mizani.formatters import comma_format, dollar_format
from plotnine import *
from siuba import *
import pandas as pd
import numpy as np
import statsmodels.api as sm
import mlxtend as mlx
#### CARGA DE DATOS ####
ames = pd.read_csv("data/ames.csv")
ames_y = ames >> pull("Sale_Price")    # ames[["Sale_Price"]]
ames_x = select(ames, -_.Sale_Price)   # ames.drop('Sale_Price', axis=1)
#### DIVISIÓN DE DATOS ####
ames_x_train, ames_x_test, ames_y_train, ames_y_test = train_test_split(
ames_x, ames_y,
test_size = 0.20,
random_state = 195
)
#### FEATURE ENGINEERING ####
## SELECCIÓN DE VARIABLES
# Seleccionamos las variales numéricas de interés
num_cols = ["Full_Bath", "Half_Bath"]
# Seleccionamos las variables categóricas de interés
cat_cols = ["Overall_Cond"]
# Juntamos todas las variables de interés
columnas_seleccionadas = num_cols + cat_cols
pipe = ColumnSelector(columnas_seleccionadas)
ames_x_train_selected = pipe.fit_transform(ames_x_train)
ames_train_selected = pd.DataFrame(
ames_x_train_selected,
columns = columnas_seleccionadas
)
ames_train_selected.info()
## TRANSFORMACIÓN DE COLUMNAS
# ColumnTransformer para aplicar transformaciones
preprocessor = ColumnTransformer(
transformers = [
('scaler', StandardScaler(), num_cols),
('onehotencoding', OneHotEncoder(drop='first'), cat_cols)
],
verbose_feature_names_out = False,
remainder = 'passthrough'  # Mantener las columnas restantes sin cambios
)
transformed_data = preprocessor.fit_transform(ames_train_selected)
new_column_names = preprocessor.get_feature_names_out()
transformed_df = pd.DataFrame(
transformed_data.todense(),
columns=new_column_names
)
transformed_df
transformed_df.info()
#### PIPELINE Y MODELADO
# Crear el pipeline con la regresión lineal
pipeline = Pipeline([
('preprocessor', preprocessor),
('regressor', LinearRegression())
])
# Entrenar el pipeline
results = pipeline.fit(ames_train_selected, ames_y_train)
## PREDICCIONES
y_pred = pipeline.predict(ames_x_test)
ames_test = (
ames_x_test >>
mutate(Sale_Price_Pred = y_pred, Sale_Price = ames_y_test)
)
ames_test.info()
(
ames_test >>
select(_.Sale_Price, _.Sale_Price_Pred)
)
##### Extracción de coeficientes
X_train_with_intercept = sm.add_constant(transformed_df)
model = sm.OLS(ames_y_train, X_train_with_intercept).fit()
model.summary()
##### Métricas de desempeño
pd.options.display.float_format = '{:.2f}'.format
y_obs = ames_test["Sale_Price"]
y_pred = ames_test["Sale_Price_Pred"]
me = np.mean(y_obs - y_pred)
mae = mean_absolute_error(y_obs, y_pred)
mape = mean_absolute_percentage_error(y_obs, y_pred)
mse = mean_squared_error(y_obs, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_obs, y_pred)
n = len(y_obs)  # Número de observaciones
p = 9  # Número de predictores
r2_adj = 1 - (n - 1) / (n - p - 1) * (1 - r2)
metrics_data = {
"Metric": ["ME", "MAE", "MAPE", "MSE", "RMSE", "R^2", "R^2 Adj"],
"Value": [me, mae, mape, mse, rmse, r2, r2_adj]
}
metrics_df = pd.DataFrame(metrics_data)
metrics_df
#### Gráficos de desempeño de modelo
(
ames_test >>
ggplot(aes(x = "Sale_Price_Pred", y = "Sale_Price")) +
geom_point() +
scale_y_continuous(labels = dollar_format(digits=0, big_mark=','), limits = [0, 600000] ) +
scale_x_continuous(labels = dollar_format(digits=0, big_mark=','), limits = [0, 500000] ) +
geom_abline(color = "red") +
coord_equal() +
labs(
title = "Comparación entre predicción y observación",
x = "Predicción",
y = "Observación")
)
(
ames_test >>
select(_.Sale_Price, _.Sale_Price_Pred) >>
mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
ggplot(aes(x = "error")) +
geom_histogram(color = "white", fill = "black") +
geom_vline(xintercept = 0, color = "red") +
scale_x_continuous(labels=dollar_format(big_mark=',', digits=0)) +
ylab("Conteos de clase") + xlab("Errores") +
ggtitle("Distribución de error")
)
(
ames_test >>
select(_.Sale_Price, _.Sale_Price_Pred) >>
mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
ggplot(aes(sample = "error")) +
geom_qq(alpha = 0.3) + stat_qq_line(color = "red") +
scale_y_continuous(labels=dollar_format(big_mark=',', digits = 0)) +
xlab("Distribución normal") + ylab("Distribución de errores") +
ggtitle("QQ-Plot")
)
(
ames_test >>
select(_.Sale_Price, _.Sale_Price_Pred) >>
mutate(error = _.Sale_Price - _.Sale_Price_Pred) >>
ggplot(aes(x = "Sale_Price")) +
geom_linerange(aes(ymin = 0, ymax = "error"), colour = "purple") +
geom_point(aes(y = "error"), size = 0.05, alpha = 0.5) +
geom_abline(intercept = 0, slope = 0) +
scale_x_continuous(labels=dollar_format(big_mark=',', digits=0)) +
scale_y_continuous(labels=dollar_format(big_mark=',', digits=0)) +
xlab("Precio real") + ylab("Error de estimación") +
ggtitle("Relación entre error y precio de venta")
)
#### Validación cruzada ####
# Definir el objeto K-Fold Cross Validator
kf = KFold(n_splits=10, shuffle=True, random_state=42)
# Definir las métricas de desempeño que deseas calcular como funciones de puntuación
scoring = {
'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
'r2': make_scorer(r2_score),
'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),
'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
}
# Realizar la validación cruzada y calcular métricas de desempeño utilizando cross_val_score
results = cross_validate(
pipeline,
ames_train_selected, ames_y_train,
cv=kf,
scoring=scoring
)
# Calcular estadísticas resumidas (media y desviación estándar) de las métricas
mean_rmse = np.mean(np.sqrt(-results['test_neg_mean_squared_error']))
std_rmse = np.std(np.sqrt(-results['test_neg_mean_squared_error']))
mean_r2 = np.mean(results['test_r2'])
std_r2 = np.std(results['test_r2'])
mean_mae = np.mean(-results['test_neg_mean_absolute_error'])
std_mae = np.std(-results['test_neg_mean_absolute_error'])
mean_mape = np.mean(-results['test_mape'])
std_mape = np.std(-results['test_mape'])
λ
# Imprimir los resultados
print(f"MAE: {mean_mae} +/- {std_mae}")
print(f"MAPE: {mean_mape} +/- {std_mape}")
print(f"R^2: {mean_r2} +/- {std_r2}")
print(f"RMSE: {mean_rmse} +/- {std_rmse}")
#| results: hide
#| fig-keep: 'all'
from mizani.formatters import comma_format, dollar_format
ames_train = (X_train >> mutate(Sale_Price= y_train))
plt.figure()
(
ames_train >> ggplot(aes(y='Sale_Price', x=0))+
geom_violin( fill='#702C27',
style='right',
alpha=0.6)+
geom_jitter(aes(x=-0.21),
size=0.8,
alpha=0.6,
color='#8E8058',
width=0.2,
random_state=7)+
geom_boxplot(aes(x=-0.21),width=0.4,
color='#182E40',
alpha=0.1,
outlier_alpha=1,
outlier_color='#235C6C',
outlier_size=1.5)+
coord_flip()+
theme_classic()+
ggtitle('Distribución de los precios de venta')+
scale_y_continuous(labels=dollar_format(big_mark=','))+
ylab('Precio de venta')+
theme(axis_ticks_minor_y=element_blank(),
axis_ticks_major_y=element_blank(),
axis_text_y=element_blank()
)+
theme(figure_size=(8, 6))
)
#| label: load-py-packages
#| include: false
import pandas as pd
from siuba import *
import numpy as np
from plotnine import *
import plydata as pr
from plydata.tidy import pivot_wider, pivot_longer
import matplotlib.pyplot as plt
import seaborn as sns
#| label: intial_split
from sklearn.model_selection import train_test_split, LeaveOneOut, cross_val_score, KFold
from sklearn.linear_model import LinearRegression
ames = pd.read_csv("../data/ames.csv")
print("Tamaño de conjunto completo: ", ames.shape)
y = ames >> pull("Sale_Price")
X = select(ames, -_.Sale_Price)
numeric_column = ames >> pull("Sale_Price")
quartiles = np.percentile(numeric_column, [25, 50, 75])
# Crea una nueva variable categórica basada en los cuartiles
stratify_variable = pd.cut(
numeric_column,
bins=[float('-inf'), quartiles[0], quartiles[1], quartiles[2], float('inf')],
labels=["Q1", "Q2", "Q3", "Q4"]
)
X_train, X_test, y_train, y_test = train_test_split(
X, y,
test_size = 0.20,
random_state = 12345,
stratify = stratify_variable
)
#| results: hide
#| fig-keep: 'all'
from mizani.formatters import comma_format, dollar_format
ames_train = (X_train >> mutate(Sale_Price= y_train))
plt.figure()
(
ames_train >> ggplot(aes(y='Sale_Price', x=0))+
geom_violin( fill='#702C27',
style='right',
alpha=0.6)+
geom_jitter(aes(x=-0.21),
size=0.8,
alpha=0.6,
color='#8E8058',
width=0.2,
random_state=7)+
geom_boxplot(aes(x=-0.21),width=0.4,
color='#182E40',
alpha=0.1,
outlier_alpha=1,
outlier_color='#235C6C',
outlier_size=1.5)+
coord_flip()+
theme_classic()+
ggtitle('Distribución de los precios de venta')+
scale_y_continuous(labels=dollar_format(big_mark=','))+
ylab('Precio de venta')+
theme(axis_ticks_minor_y=element_blank(),
axis_ticks_major_y=element_blank(),
axis_text_y=element_blank()
)+
theme(figure_size=(8, 6))
)
#| label: load-py-packages
#| include: false
import pandas as pd
from siuba import *
import numpy as np
from plotnine import *
import plydata as pr
from plydata.tidy import pivot_wider, pivot_longer
import matplotlib.pyplot as plt
import seaborn as sns
pull("Sale_Price")
ames >> pull("Sale_Price")
